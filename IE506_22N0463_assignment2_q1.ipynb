{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(a)}$"
      ],
      "metadata": {
        "id": "byK2Uesr4Zzs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rM6-ILGbmMKo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kQ3lW3j3r9rZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open('Data_Q1.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Initialize empty lists to hold the labels and features\n",
        "labels = []\n",
        "features = []\n",
        "\n",
        "# Loop through each line in the file\n",
        "for line in lines:\n",
        "    # Split the line into its label and feature components\n",
        "    parts = line.strip().split(' ')\n",
        "    label = int(parts[0])\n",
        "    feature_strs = parts[1:]\n",
        "\n",
        "    # Initialize an empty dictionary to hold the feature values\n",
        "    feature_dict = {}\n",
        "\n",
        "    # Loop through each feature in the line and add it to the dictionary\n",
        "    for feature_str in feature_strs:\n",
        "        feature_parts = feature_str.split(':')\n",
        "        feature_id = int(feature_parts[0])\n",
        "        feature_val = float(feature_parts[1])\n",
        "        feature_dict[feature_id] = feature_val\n",
        "\n",
        "    # Append the label and feature dictionary to their respective lists\n",
        "    labels.append(label)\n",
        "    features.append(feature_dict)\n",
        "\n",
        "# Determine the maximum feature ID in the dataset\n",
        "max_feature_id = max(max(feature_dict.keys()) for feature_dict in features)\n",
        "\n",
        "# Initialize a numpy array of zeros to hold the feature values\n",
        "X = np.zeros((len(features), max_feature_id))\n",
        "\n",
        "# Loop through each sample and set its feature values in the X array\n",
        "for i, feature_dict in enumerate(features):\n",
        "    for feature_id, feature_val in feature_dict.items():\n",
        "        X[i, feature_id-1] = feature_val\n",
        "\n",
        "# Convert the labels list to a numpy array\n",
        "y = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0409phCsBdl",
        "outputId": "c90d291a-1a0e-4244-dfed-aca8caa19cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeei1CytutAj",
        "outputId": "f4ce675d-c563-46f0-ad9d-d3cc23fbf5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1 -1 -1 ...  1 -1  1]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCvx62VHim7C",
        "outputId": "c0b4d88c-b1ed-4a56-aad7-3aa6e85fdab1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4143, 54877)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, the dataset is toolarge, we'll apply dimension reduction technique i.e., PCA to reduce the dimension of the dataset."
      ],
      "metadata": {
        "id": "g1jWe_IUipjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "my_model = PCA(n_components=520)\n",
        "X_new=my_model.fit_transform(X)"
      ],
      "metadata": {
        "id": "SIIKb5uQaH8P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print (my_model.explained_variance_)\n",
        "# print (my_model.explained_variance_ratio_)\n",
        "print (my_model.explained_variance_ratio_.cumsum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKXWGH_vaegN",
        "outputId": "70ea6e50-8808-4d10-8de9-41dcb30a8df2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06151249 0.10554605 0.1291997  0.14915661 0.16583007 0.17891754\n",
            " 0.19130718 0.20264855 0.2128589  0.22274433 0.23229439 0.24062825\n",
            " 0.2482886  0.25581608 0.26315092 0.26967828 0.27586312 0.28186658\n",
            " 0.2876791  0.29331067 0.29892204 0.30427433 0.30956145 0.31472194\n",
            " 0.31966877 0.32443239 0.3289975  0.33352215 0.33797888 0.34224001\n",
            " 0.34643139 0.35059192 0.3546258  0.35854086 0.36240983 0.36623747\n",
            " 0.3699218  0.37357193 0.37711375 0.38056736 0.38399015 0.38733268\n",
            " 0.39064541 0.39388086 0.39708878 0.40021163 0.40328667 0.40633915\n",
            " 0.40936083 0.41232849 0.41528644 0.41815654 0.42097173 0.42375116\n",
            " 0.42643715 0.42908486 0.43169565 0.43423838 0.43675617 0.43927008\n",
            " 0.44175135 0.44418376 0.44661012 0.44897991 0.45132254 0.45365853\n",
            " 0.45598168 0.45822013 0.46044395 0.46263343 0.46480956 0.46695911\n",
            " 0.46907389 0.47116247 0.47323623 0.47526996 0.47728671 0.4792883\n",
            " 0.48127499 0.48324654 0.48520248 0.48714164 0.4890487  0.49094193\n",
            " 0.49282388 0.49465412 0.49646814 0.4982701  0.50006522 0.50184974\n",
            " 0.50362337 0.50537253 0.50711074 0.5088269  0.51052907 0.51222056\n",
            " 0.51390104 0.51556395 0.51722366 0.51886225 0.52048065 0.52208004\n",
            " 0.52367508 0.52525244 0.52681251 0.52836855 0.52990916 0.53142308\n",
            " 0.53293196 0.53443853 0.5359154  0.53738142 0.53884134 0.54028503\n",
            " 0.54172348 0.54315617 0.54457134 0.54598043 0.54737141 0.54875825\n",
            " 0.5501396  0.55149948 0.55285269 0.55420127 0.55554596 0.5568824\n",
            " 0.55821149 0.55952826 0.56084001 0.56212307 0.56339757 0.56466682\n",
            " 0.56592245 0.56717647 0.56842243 0.56965911 0.57089018 0.57211385\n",
            " 0.57332336 0.574531   0.57572556 0.57691778 0.57810422 0.57927574\n",
            " 0.58043898 0.58159839 0.58274985 0.58388478 0.58501275 0.58613421\n",
            " 0.58725166 0.58836254 0.58947019 0.59056537 0.59165437 0.59273836\n",
            " 0.59381712 0.59488861 0.59594345 0.59699356 0.59804065 0.59908581\n",
            " 0.6001181  0.60114496 0.60216783 0.60318244 0.60419489 0.60520108\n",
            " 0.60620268 0.60719664 0.6081863  0.60917244 0.6101505  0.61112141\n",
            " 0.61209061 0.61305278 0.61400954 0.61496384 0.61591387 0.61685935\n",
            " 0.61780281 0.61874015 0.61967658 0.62060458 0.62153219 0.62244923\n",
            " 0.62336296 0.62427361 0.6251788  0.62608267 0.62697986 0.62787524\n",
            " 0.62875848 0.62964018 0.63052066 0.63139946 0.63227526 0.63314234\n",
            " 0.63400745 0.63486703 0.63572644 0.63658001 0.63743101 0.63828044\n",
            " 0.63912252 0.63995981 0.64079544 0.64162372 0.64244936 0.64327479\n",
            " 0.64409568 0.64491188 0.64572559 0.64653137 0.64733583 0.64813709\n",
            " 0.648935   0.64973062 0.65052461 0.65131533 0.6521038  0.652888\n",
            " 0.653668   0.6544451  0.65521857 0.65599051 0.65675991 0.65752583\n",
            " 0.6582895  0.6590468  0.65980042 0.66055054 0.66129782 0.66203997\n",
            " 0.66278141 0.66351931 0.6642535  0.66498637 0.66571703 0.6664473\n",
            " 0.66717474 0.66789749 0.6686186  0.66933654 0.67005388 0.67076332\n",
            " 0.67147236 0.67217738 0.6728812  0.67358077 0.67427876 0.67497365\n",
            " 0.67566539 0.67635464 0.67704209 0.67772838 0.67841388 0.6790966\n",
            " 0.67977819 0.68045499 0.68112728 0.68179907 0.68246604 0.68313185\n",
            " 0.68379585 0.68445836 0.68511887 0.68577887 0.68643449 0.68708881\n",
            " 0.68774166 0.68839185 0.68904087 0.68968587 0.69032765 0.6909689\n",
            " 0.69160778 0.6922451  0.69287911 0.69351104 0.69414038 0.69476838\n",
            " 0.69539256 0.69601404 0.69663505 0.69725531 0.69787119 0.69848648\n",
            " 0.69909874 0.69970806 0.7003171  0.70092496 0.70153031 0.7021343\n",
            " 0.70273569 0.70333627 0.70393562 0.70453023 0.70512219 0.70571253\n",
            " 0.70630153 0.70688953 0.70747448 0.70805791 0.70863873 0.70921769\n",
            " 0.70979602 0.71037076 0.7109444  0.71151724 0.71208841 0.71265785\n",
            " 0.71322363 0.71378819 0.71435147 0.71491288 0.71547272 0.7160298\n",
            " 0.71658491 0.71713887 0.71769027 0.71824055 0.71879015 0.7193378\n",
            " 0.71988495 0.72042974 0.72097344 0.72151589 0.72205685 0.72259539\n",
            " 0.72313181 0.72366759 0.72420051 0.72473091 0.72526018 0.72578856\n",
            " 0.72631582 0.72684188 0.72736642 0.72788935 0.72841092 0.72892988\n",
            " 0.72944779 0.72996441 0.7304788  0.73099084 0.73150197 0.73201237\n",
            " 0.73252171 0.73303033 0.73353759 0.7340419  0.7345453  0.73504718\n",
            " 0.73554835 0.73604868 0.73654788 0.73704438 0.73753964 0.73803294\n",
            " 0.73852501 0.7390164  0.73950657 0.73999455 0.7404795  0.74096368\n",
            " 0.7414466  0.74192838 0.74240892 0.74288794 0.74336535 0.74384062\n",
            " 0.74431546 0.74478907 0.74526111 0.74573201 0.74620145 0.74666911\n",
            " 0.74713545 0.74760099 0.74806599 0.74852972 0.7489917  0.74945303\n",
            " 0.74991282 0.75037084 0.75082742 0.75128191 0.75173496 0.75218776\n",
            " 0.75263878 0.75308891 0.75353785 0.75398586 0.75443257 0.75487782\n",
            " 0.75532248 0.7557652  0.75620575 0.75664533 0.757084   0.75752101\n",
            " 0.75795715 0.75839314 0.75882734 0.7592602  0.75969195 0.76012249\n",
            " 0.76055273 0.76098087 0.76140796 0.76183354 0.76225813 0.76268192\n",
            " 0.76310451 0.76352667 0.76394636 0.76436464 0.76478153 0.76519755\n",
            " 0.76561189 0.76602554 0.76643914 0.76685021 0.76725905 0.7676663\n",
            " 0.76807289 0.76847803 0.76888208 0.76928529 0.76968758 0.77008932\n",
            " 0.77048889 0.77088713 0.77128446 0.77168068 0.77207652 0.77247152\n",
            " 0.77286555 0.77325825 0.77365042 0.7740396  0.77442826 0.77481626\n",
            " 0.77520381 0.7755906  0.77597609 0.77636086 0.77674443 0.77712702\n",
            " 0.7775078  0.77788744 0.77826611 0.77864375 0.77902072 0.77939673\n",
            " 0.77977106 0.78014528 0.78051783 0.78088941 0.78125929 0.78162748\n",
            " 0.78199545 0.7823621  0.78272745 0.78309134 0.78345432 0.78381614\n",
            " 0.78417705 0.78453729 0.78489605 0.78525366 0.78561092 0.78596609\n",
            " 0.78632017 0.78667383 0.78702659 0.7873782  0.78772923 0.78807912\n",
            " 0.78842838 0.7887761  0.78912287 0.78946852 0.78981331 0.79015798\n",
            " 0.79050068 0.79084182 0.79118173 0.79152048 0.79185813 0.79219489\n",
            " 0.79253055 0.79286455 0.79319773 0.79352912 0.79385979 0.79418932\n",
            " 0.7945183  0.79484618 0.79517183 0.79549683 0.79582034 0.79614254\n",
            " 0.79646377 0.79678422 0.79710281 0.79742064 0.79773711 0.79805297\n",
            " 0.79836792 0.79868192 0.79899495 0.79930709 0.79961648 0.7999242\n",
            " 0.80023004 0.80053495 0.80083803 0.80114063]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO6IXkEphnkF",
        "outputId": "ac07b852-4947-4941-c913-ccb6375f8f15"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.16355559e+00,  1.35449233e+00,  1.22609270e-01, ...,\n",
              "        -9.90037988e-04, -5.49364913e-03, -2.67328444e-02],\n",
              "       [ 2.94461102e+00, -1.85303372e+00, -3.42356929e+00, ...,\n",
              "        -9.31028792e-02, -1.07548775e-01, -3.16967337e-02],\n",
              "       [-2.42472556e+00,  1.07460981e+00, -9.15473282e-02, ...,\n",
              "        -2.45097649e-02, -2.26305099e-01,  1.23648802e-01],\n",
              "       ...,\n",
              "       [ 8.89478955e-01, -9.01727203e-01,  2.04859232e+00, ...,\n",
              "        -2.58756973e-01, -4.81303517e-01,  1.05033025e+00],\n",
              "       [-1.75517906e+00,  5.72070547e-01,  1.17252505e-01, ...,\n",
              "         9.38607214e-02,  6.72003502e-02, -8.24270882e-02],\n",
              "       [ 1.22167361e+01,  1.40410531e+01, -6.38559838e-01, ...,\n",
              "         6.23316654e-02,  1.35446900e-01, -1.84815462e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(X_new)"
      ],
      "metadata": {
        "id": "kZWroVj0eYu7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1qsFJ6h7eCn3",
        "outputId": "474e3cfa-1508-4552-8734-f6a48db51ae5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0          1         2         3         4         5         6    \\\n",
              "0     -3.163556   1.354492  0.122609  0.614129 -0.325644 -0.765679 -0.197722   \n",
              "1      2.944611  -1.853034 -3.423569 -3.943277 -3.041524  3.619496 -3.783854   \n",
              "2     -2.424726   1.074610 -0.091547  0.004458 -0.401874 -0.567940 -0.354899   \n",
              "3     -1.785259   0.864062  0.235106  0.025431  0.331659  0.115276 -0.176223   \n",
              "4     -3.167082   1.337177  0.047686  0.561414 -0.369034 -0.765479 -0.196355   \n",
              "...         ...        ...       ...       ...       ...       ...       ...   \n",
              "4138  -1.446324   0.642952 -0.330377 -0.011503 -0.000712 -0.501163  0.366723   \n",
              "4139  -0.368333   0.016102  0.309881  0.342469 -0.103763 -0.023977  0.059648   \n",
              "4140   0.889479  -0.901727  2.048592  0.134772 -0.032090  0.193295  0.155847   \n",
              "4141  -1.755179   0.572071  0.117253  0.273678 -0.116884 -0.404077 -0.023015   \n",
              "4142  12.216736  14.041053 -0.638560  0.219674  0.336044 -0.271704 -0.013033   \n",
              "\n",
              "           7         8         9    ...       510       511       512  \\\n",
              "0     0.345623 -0.583531 -0.061195  ...  0.003820  0.060101  0.020149   \n",
              "1    -4.853894 -5.449297  0.604696  ... -0.052582  0.082848 -0.001395   \n",
              "2     0.011724 -0.074278  0.295060  ...  0.044600 -0.070424 -0.009781   \n",
              "3    -0.279108 -0.062008  0.453530  ...  0.021739 -0.011732  0.001054   \n",
              "4     0.298536 -0.632671 -0.046335  ... -0.099963 -0.049586 -0.084686   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "4138  0.138004  0.183987  0.713666  ...  0.164399  0.415125  0.233824   \n",
              "4139 -0.108008  0.518320  0.090268  ...  0.115440  0.166630 -0.113661   \n",
              "4140 -0.452384  1.554877 -0.544347  ... -0.421160  0.471329  0.179395   \n",
              "4141  0.529717 -0.194253  0.237860  ...  0.197197  0.154660  0.112264   \n",
              "4142  0.106316 -0.181107 -0.102531  ... -0.098193 -0.261336  0.044714   \n",
              "\n",
              "           513       514       515       516       517       518       519  \n",
              "0     0.023405 -0.038304  0.009178  0.000593 -0.000990 -0.005494 -0.026733  \n",
              "1     0.011221 -0.067226  0.023677 -0.154949 -0.093103 -0.107549 -0.031697  \n",
              "2     0.070656  0.060470  0.023062  0.052919 -0.024510 -0.226305  0.123649  \n",
              "3     0.008765 -0.032580 -0.012291 -0.008854 -0.028305 -0.024816 -0.004646  \n",
              "4    -0.016154 -0.056588  0.039681 -0.058456 -0.039702  0.032842  0.087111  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "4138  0.007281 -0.318274 -0.178287  0.031619  0.074725  0.139038  0.362159  \n",
              "4139  0.204370  0.007903 -0.054292 -0.063204  0.054268 -0.054584  0.080094  \n",
              "4140 -0.424239  1.195420  1.178119 -0.078330 -0.258757 -0.481304  1.050330  \n",
              "4141 -0.135992  0.106883 -0.062000  0.174739  0.093861  0.067200 -0.082427  \n",
              "4142  0.028948  0.005105  0.117186  0.020397  0.062332  0.135447 -0.184815  \n",
              "\n",
              "[4143 rows x 520 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90778e56-1add-4d2e-ab91-461d3520871b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-3.163556</td>\n",
              "      <td>1.354492</td>\n",
              "      <td>0.122609</td>\n",
              "      <td>0.614129</td>\n",
              "      <td>-0.325644</td>\n",
              "      <td>-0.765679</td>\n",
              "      <td>-0.197722</td>\n",
              "      <td>0.345623</td>\n",
              "      <td>-0.583531</td>\n",
              "      <td>-0.061195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>0.060101</td>\n",
              "      <td>0.020149</td>\n",
              "      <td>0.023405</td>\n",
              "      <td>-0.038304</td>\n",
              "      <td>0.009178</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>-0.000990</td>\n",
              "      <td>-0.005494</td>\n",
              "      <td>-0.026733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.944611</td>\n",
              "      <td>-1.853034</td>\n",
              "      <td>-3.423569</td>\n",
              "      <td>-3.943277</td>\n",
              "      <td>-3.041524</td>\n",
              "      <td>3.619496</td>\n",
              "      <td>-3.783854</td>\n",
              "      <td>-4.853894</td>\n",
              "      <td>-5.449297</td>\n",
              "      <td>0.604696</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052582</td>\n",
              "      <td>0.082848</td>\n",
              "      <td>-0.001395</td>\n",
              "      <td>0.011221</td>\n",
              "      <td>-0.067226</td>\n",
              "      <td>0.023677</td>\n",
              "      <td>-0.154949</td>\n",
              "      <td>-0.093103</td>\n",
              "      <td>-0.107549</td>\n",
              "      <td>-0.031697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.424726</td>\n",
              "      <td>1.074610</td>\n",
              "      <td>-0.091547</td>\n",
              "      <td>0.004458</td>\n",
              "      <td>-0.401874</td>\n",
              "      <td>-0.567940</td>\n",
              "      <td>-0.354899</td>\n",
              "      <td>0.011724</td>\n",
              "      <td>-0.074278</td>\n",
              "      <td>0.295060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044600</td>\n",
              "      <td>-0.070424</td>\n",
              "      <td>-0.009781</td>\n",
              "      <td>0.070656</td>\n",
              "      <td>0.060470</td>\n",
              "      <td>0.023062</td>\n",
              "      <td>0.052919</td>\n",
              "      <td>-0.024510</td>\n",
              "      <td>-0.226305</td>\n",
              "      <td>0.123649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.785259</td>\n",
              "      <td>0.864062</td>\n",
              "      <td>0.235106</td>\n",
              "      <td>0.025431</td>\n",
              "      <td>0.331659</td>\n",
              "      <td>0.115276</td>\n",
              "      <td>-0.176223</td>\n",
              "      <td>-0.279108</td>\n",
              "      <td>-0.062008</td>\n",
              "      <td>0.453530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>-0.011732</td>\n",
              "      <td>0.001054</td>\n",
              "      <td>0.008765</td>\n",
              "      <td>-0.032580</td>\n",
              "      <td>-0.012291</td>\n",
              "      <td>-0.008854</td>\n",
              "      <td>-0.028305</td>\n",
              "      <td>-0.024816</td>\n",
              "      <td>-0.004646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.167082</td>\n",
              "      <td>1.337177</td>\n",
              "      <td>0.047686</td>\n",
              "      <td>0.561414</td>\n",
              "      <td>-0.369034</td>\n",
              "      <td>-0.765479</td>\n",
              "      <td>-0.196355</td>\n",
              "      <td>0.298536</td>\n",
              "      <td>-0.632671</td>\n",
              "      <td>-0.046335</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.099963</td>\n",
              "      <td>-0.049586</td>\n",
              "      <td>-0.084686</td>\n",
              "      <td>-0.016154</td>\n",
              "      <td>-0.056588</td>\n",
              "      <td>0.039681</td>\n",
              "      <td>-0.058456</td>\n",
              "      <td>-0.039702</td>\n",
              "      <td>0.032842</td>\n",
              "      <td>0.087111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4138</th>\n",
              "      <td>-1.446324</td>\n",
              "      <td>0.642952</td>\n",
              "      <td>-0.330377</td>\n",
              "      <td>-0.011503</td>\n",
              "      <td>-0.000712</td>\n",
              "      <td>-0.501163</td>\n",
              "      <td>0.366723</td>\n",
              "      <td>0.138004</td>\n",
              "      <td>0.183987</td>\n",
              "      <td>0.713666</td>\n",
              "      <td>...</td>\n",
              "      <td>0.164399</td>\n",
              "      <td>0.415125</td>\n",
              "      <td>0.233824</td>\n",
              "      <td>0.007281</td>\n",
              "      <td>-0.318274</td>\n",
              "      <td>-0.178287</td>\n",
              "      <td>0.031619</td>\n",
              "      <td>0.074725</td>\n",
              "      <td>0.139038</td>\n",
              "      <td>0.362159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4139</th>\n",
              "      <td>-0.368333</td>\n",
              "      <td>0.016102</td>\n",
              "      <td>0.309881</td>\n",
              "      <td>0.342469</td>\n",
              "      <td>-0.103763</td>\n",
              "      <td>-0.023977</td>\n",
              "      <td>0.059648</td>\n",
              "      <td>-0.108008</td>\n",
              "      <td>0.518320</td>\n",
              "      <td>0.090268</td>\n",
              "      <td>...</td>\n",
              "      <td>0.115440</td>\n",
              "      <td>0.166630</td>\n",
              "      <td>-0.113661</td>\n",
              "      <td>0.204370</td>\n",
              "      <td>0.007903</td>\n",
              "      <td>-0.054292</td>\n",
              "      <td>-0.063204</td>\n",
              "      <td>0.054268</td>\n",
              "      <td>-0.054584</td>\n",
              "      <td>0.080094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4140</th>\n",
              "      <td>0.889479</td>\n",
              "      <td>-0.901727</td>\n",
              "      <td>2.048592</td>\n",
              "      <td>0.134772</td>\n",
              "      <td>-0.032090</td>\n",
              "      <td>0.193295</td>\n",
              "      <td>0.155847</td>\n",
              "      <td>-0.452384</td>\n",
              "      <td>1.554877</td>\n",
              "      <td>-0.544347</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.421160</td>\n",
              "      <td>0.471329</td>\n",
              "      <td>0.179395</td>\n",
              "      <td>-0.424239</td>\n",
              "      <td>1.195420</td>\n",
              "      <td>1.178119</td>\n",
              "      <td>-0.078330</td>\n",
              "      <td>-0.258757</td>\n",
              "      <td>-0.481304</td>\n",
              "      <td>1.050330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4141</th>\n",
              "      <td>-1.755179</td>\n",
              "      <td>0.572071</td>\n",
              "      <td>0.117253</td>\n",
              "      <td>0.273678</td>\n",
              "      <td>-0.116884</td>\n",
              "      <td>-0.404077</td>\n",
              "      <td>-0.023015</td>\n",
              "      <td>0.529717</td>\n",
              "      <td>-0.194253</td>\n",
              "      <td>0.237860</td>\n",
              "      <td>...</td>\n",
              "      <td>0.197197</td>\n",
              "      <td>0.154660</td>\n",
              "      <td>0.112264</td>\n",
              "      <td>-0.135992</td>\n",
              "      <td>0.106883</td>\n",
              "      <td>-0.062000</td>\n",
              "      <td>0.174739</td>\n",
              "      <td>0.093861</td>\n",
              "      <td>0.067200</td>\n",
              "      <td>-0.082427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4142</th>\n",
              "      <td>12.216736</td>\n",
              "      <td>14.041053</td>\n",
              "      <td>-0.638560</td>\n",
              "      <td>0.219674</td>\n",
              "      <td>0.336044</td>\n",
              "      <td>-0.271704</td>\n",
              "      <td>-0.013033</td>\n",
              "      <td>0.106316</td>\n",
              "      <td>-0.181107</td>\n",
              "      <td>-0.102531</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.098193</td>\n",
              "      <td>-0.261336</td>\n",
              "      <td>0.044714</td>\n",
              "      <td>0.028948</td>\n",
              "      <td>0.005105</td>\n",
              "      <td>0.117186</td>\n",
              "      <td>0.020397</td>\n",
              "      <td>0.062332</td>\n",
              "      <td>0.135447</td>\n",
              "      <td>-0.184815</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4143 rows × 520 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90778e56-1add-4d2e-ab91-461d3520871b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90778e56-1add-4d2e-ab91-461d3520871b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90778e56-1add-4d2e-ab91-461d3520871b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XIkG2GviQ6Q",
        "outputId": "8ebefe01-d643-43f8-e5e1-db22c12f9cc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.16355559e+00,  1.35449233e+00,  1.22609270e-01, ...,\n",
              "        -9.90037988e-04, -5.49364913e-03, -2.67328444e-02],\n",
              "       [ 2.94461102e+00, -1.85303372e+00, -3.42356929e+00, ...,\n",
              "        -9.31028792e-02, -1.07548775e-01, -3.16967337e-02],\n",
              "       [-2.42472556e+00,  1.07460981e+00, -9.15473282e-02, ...,\n",
              "        -2.45097649e-02, -2.26305099e-01,  1.23648802e-01],\n",
              "       ...,\n",
              "       [ 8.89478955e-01, -9.01727203e-01,  2.04859232e+00, ...,\n",
              "        -2.58756973e-01, -4.81303517e-01,  1.05033025e+00],\n",
              "       [-1.75517906e+00,  5.72070547e-01,  1.17252505e-01, ...,\n",
              "         9.38607214e-02,  6.72003502e-02, -8.24270882e-02],\n",
              "       [ 1.22167361e+01,  1.40410531e+01, -6.38559838e-01, ...,\n",
              "         6.23316654e-02,  1.35446900e-01, -1.84815462e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i64ae9oasJlh"
      },
      "outputs": [],
      "source": [
        "# df = pd.DataFrame(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "igiAUTpbsPKG"
      },
      "outputs": [],
      "source": [
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WM1KPgDFuwZV"
      },
      "outputs": [],
      "source": [
        "df.insert(0, 'Labels', y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6OoiSnZEvWng",
        "outputId": "469e7263-1729-4fbc-b05d-bcc7898bc905"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Labels         0         1         2         3         4         5  \\\n",
              "0       1 -3.163556  1.354492  0.122609  0.614129 -0.325644 -0.765679   \n",
              "1      -1  2.944611 -1.853034 -3.423569 -3.943277 -3.041524  3.619496   \n",
              "2      -1 -2.424726  1.074610 -0.091547  0.004458 -0.401874 -0.567940   \n",
              "3      -1 -1.785259  0.864062  0.235106  0.025431  0.331659  0.115276   \n",
              "4      -1 -3.167082  1.337177  0.047686  0.561414 -0.369034 -0.765479   \n",
              "5       1 -3.173292  1.359495  0.126549  0.621061 -0.324827 -0.770202   \n",
              "6      -1 -3.151312  1.341245  0.126068  0.557534 -0.350916 -0.762246   \n",
              "7      -1 -0.041639 -1.157616 -2.948790 -1.464512  0.698901 -1.345332   \n",
              "8      -1 -1.377654  0.057790 -0.308212 -0.176832 -0.001134 -0.371665   \n",
              "9       1 -0.668844  0.056571 -0.213935  0.383677 -0.565103  0.149276   \n",
              "\n",
              "          6         7         8  ...       510       511       512       513  \\\n",
              "0 -0.197722  0.345623 -0.583531  ...  0.003820  0.060101  0.020149  0.023405   \n",
              "1 -3.783854 -4.853894 -5.449297  ... -0.052582  0.082848 -0.001395  0.011221   \n",
              "2 -0.354899  0.011724 -0.074278  ...  0.044600 -0.070424 -0.009781  0.070656   \n",
              "3 -0.176223 -0.279108 -0.062008  ...  0.021739 -0.011732  0.001054  0.008765   \n",
              "4 -0.196355  0.298536 -0.632671  ... -0.099963 -0.049586 -0.084686 -0.016154   \n",
              "5 -0.199446  0.347291 -0.585765  ... -0.022517  0.101262  0.015745  0.017342   \n",
              "6 -0.144797  0.259317 -0.565195  ...  0.011294  0.103082 -0.054937  0.027524   \n",
              "7 -1.400983  2.207661  1.297725  ... -0.000088  0.001768  0.036919  0.036221   \n",
              "8 -0.268557  0.482490 -0.062888  ... -0.428070  0.314901 -0.467783  0.160548   \n",
              "9 -0.061215  0.308903 -0.469805  ...  0.292289 -0.173074 -0.201735  0.009236   \n",
              "\n",
              "        514       515       516       517       518       519  \n",
              "0 -0.038304  0.009178  0.000593 -0.000990 -0.005494 -0.026733  \n",
              "1 -0.067226  0.023677 -0.154949 -0.093103 -0.107549 -0.031697  \n",
              "2  0.060470  0.023062  0.052919 -0.024510 -0.226305  0.123649  \n",
              "3 -0.032580 -0.012291 -0.008854 -0.028305 -0.024816 -0.004646  \n",
              "4 -0.056588  0.039681 -0.058456 -0.039702  0.032842  0.087111  \n",
              "5 -0.075561  0.066350 -0.025293 -0.005848 -0.004052 -0.025131  \n",
              "6 -0.034878 -0.001380 -0.041615 -0.067991  0.042516  0.017005  \n",
              "7  0.005923  0.002326  0.014178 -0.013862  0.005604  0.000212  \n",
              "8 -0.035823  0.533930  0.155909 -0.091103 -0.233084 -0.101171  \n",
              "9 -0.343089 -0.088519 -0.096788  0.141411  0.100367 -0.244700  \n",
              "\n",
              "[10 rows x 521 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a12bf10-b5d7-4ebe-8d2b-be7b28ca8a27\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.163556</td>\n",
              "      <td>1.354492</td>\n",
              "      <td>0.122609</td>\n",
              "      <td>0.614129</td>\n",
              "      <td>-0.325644</td>\n",
              "      <td>-0.765679</td>\n",
              "      <td>-0.197722</td>\n",
              "      <td>0.345623</td>\n",
              "      <td>-0.583531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>0.060101</td>\n",
              "      <td>0.020149</td>\n",
              "      <td>0.023405</td>\n",
              "      <td>-0.038304</td>\n",
              "      <td>0.009178</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>-0.000990</td>\n",
              "      <td>-0.005494</td>\n",
              "      <td>-0.026733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>2.944611</td>\n",
              "      <td>-1.853034</td>\n",
              "      <td>-3.423569</td>\n",
              "      <td>-3.943277</td>\n",
              "      <td>-3.041524</td>\n",
              "      <td>3.619496</td>\n",
              "      <td>-3.783854</td>\n",
              "      <td>-4.853894</td>\n",
              "      <td>-5.449297</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052582</td>\n",
              "      <td>0.082848</td>\n",
              "      <td>-0.001395</td>\n",
              "      <td>0.011221</td>\n",
              "      <td>-0.067226</td>\n",
              "      <td>0.023677</td>\n",
              "      <td>-0.154949</td>\n",
              "      <td>-0.093103</td>\n",
              "      <td>-0.107549</td>\n",
              "      <td>-0.031697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>-2.424726</td>\n",
              "      <td>1.074610</td>\n",
              "      <td>-0.091547</td>\n",
              "      <td>0.004458</td>\n",
              "      <td>-0.401874</td>\n",
              "      <td>-0.567940</td>\n",
              "      <td>-0.354899</td>\n",
              "      <td>0.011724</td>\n",
              "      <td>-0.074278</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044600</td>\n",
              "      <td>-0.070424</td>\n",
              "      <td>-0.009781</td>\n",
              "      <td>0.070656</td>\n",
              "      <td>0.060470</td>\n",
              "      <td>0.023062</td>\n",
              "      <td>0.052919</td>\n",
              "      <td>-0.024510</td>\n",
              "      <td>-0.226305</td>\n",
              "      <td>0.123649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.785259</td>\n",
              "      <td>0.864062</td>\n",
              "      <td>0.235106</td>\n",
              "      <td>0.025431</td>\n",
              "      <td>0.331659</td>\n",
              "      <td>0.115276</td>\n",
              "      <td>-0.176223</td>\n",
              "      <td>-0.279108</td>\n",
              "      <td>-0.062008</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>-0.011732</td>\n",
              "      <td>0.001054</td>\n",
              "      <td>0.008765</td>\n",
              "      <td>-0.032580</td>\n",
              "      <td>-0.012291</td>\n",
              "      <td>-0.008854</td>\n",
              "      <td>-0.028305</td>\n",
              "      <td>-0.024816</td>\n",
              "      <td>-0.004646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>-3.167082</td>\n",
              "      <td>1.337177</td>\n",
              "      <td>0.047686</td>\n",
              "      <td>0.561414</td>\n",
              "      <td>-0.369034</td>\n",
              "      <td>-0.765479</td>\n",
              "      <td>-0.196355</td>\n",
              "      <td>0.298536</td>\n",
              "      <td>-0.632671</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.099963</td>\n",
              "      <td>-0.049586</td>\n",
              "      <td>-0.084686</td>\n",
              "      <td>-0.016154</td>\n",
              "      <td>-0.056588</td>\n",
              "      <td>0.039681</td>\n",
              "      <td>-0.058456</td>\n",
              "      <td>-0.039702</td>\n",
              "      <td>0.032842</td>\n",
              "      <td>0.087111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.173292</td>\n",
              "      <td>1.359495</td>\n",
              "      <td>0.126549</td>\n",
              "      <td>0.621061</td>\n",
              "      <td>-0.324827</td>\n",
              "      <td>-0.770202</td>\n",
              "      <td>-0.199446</td>\n",
              "      <td>0.347291</td>\n",
              "      <td>-0.585765</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.022517</td>\n",
              "      <td>0.101262</td>\n",
              "      <td>0.015745</td>\n",
              "      <td>0.017342</td>\n",
              "      <td>-0.075561</td>\n",
              "      <td>0.066350</td>\n",
              "      <td>-0.025293</td>\n",
              "      <td>-0.005848</td>\n",
              "      <td>-0.004052</td>\n",
              "      <td>-0.025131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>-3.151312</td>\n",
              "      <td>1.341245</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.557534</td>\n",
              "      <td>-0.350916</td>\n",
              "      <td>-0.762246</td>\n",
              "      <td>-0.144797</td>\n",
              "      <td>0.259317</td>\n",
              "      <td>-0.565195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011294</td>\n",
              "      <td>0.103082</td>\n",
              "      <td>-0.054937</td>\n",
              "      <td>0.027524</td>\n",
              "      <td>-0.034878</td>\n",
              "      <td>-0.001380</td>\n",
              "      <td>-0.041615</td>\n",
              "      <td>-0.067991</td>\n",
              "      <td>0.042516</td>\n",
              "      <td>0.017005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>-0.041639</td>\n",
              "      <td>-1.157616</td>\n",
              "      <td>-2.948790</td>\n",
              "      <td>-1.464512</td>\n",
              "      <td>0.698901</td>\n",
              "      <td>-1.345332</td>\n",
              "      <td>-1.400983</td>\n",
              "      <td>2.207661</td>\n",
              "      <td>1.297725</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.001768</td>\n",
              "      <td>0.036919</td>\n",
              "      <td>0.036221</td>\n",
              "      <td>0.005923</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>0.014178</td>\n",
              "      <td>-0.013862</td>\n",
              "      <td>0.005604</td>\n",
              "      <td>0.000212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.377654</td>\n",
              "      <td>0.057790</td>\n",
              "      <td>-0.308212</td>\n",
              "      <td>-0.176832</td>\n",
              "      <td>-0.001134</td>\n",
              "      <td>-0.371665</td>\n",
              "      <td>-0.268557</td>\n",
              "      <td>0.482490</td>\n",
              "      <td>-0.062888</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.428070</td>\n",
              "      <td>0.314901</td>\n",
              "      <td>-0.467783</td>\n",
              "      <td>0.160548</td>\n",
              "      <td>-0.035823</td>\n",
              "      <td>0.533930</td>\n",
              "      <td>0.155909</td>\n",
              "      <td>-0.091103</td>\n",
              "      <td>-0.233084</td>\n",
              "      <td>-0.101171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.668844</td>\n",
              "      <td>0.056571</td>\n",
              "      <td>-0.213935</td>\n",
              "      <td>0.383677</td>\n",
              "      <td>-0.565103</td>\n",
              "      <td>0.149276</td>\n",
              "      <td>-0.061215</td>\n",
              "      <td>0.308903</td>\n",
              "      <td>-0.469805</td>\n",
              "      <td>...</td>\n",
              "      <td>0.292289</td>\n",
              "      <td>-0.173074</td>\n",
              "      <td>-0.201735</td>\n",
              "      <td>0.009236</td>\n",
              "      <td>-0.343089</td>\n",
              "      <td>-0.088519</td>\n",
              "      <td>-0.096788</td>\n",
              "      <td>0.141411</td>\n",
              "      <td>0.100367</td>\n",
              "      <td>-0.244700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 521 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a12bf10-b5d7-4ebe-8d2b-be7b28ca8a27')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a12bf10-b5d7-4ebe-8d2b-be7b28ca8a27 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a12bf10-b5d7-4ebe-8d2b-be7b28ca8a27');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(b)}$"
      ],
      "metadata": {
        "id": "qq8U2E9L5dZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Labels'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4wKB595I6N",
        "outputId": "46469a4f-baed-4a1f-9771-51cecb7c148f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    2210\n",
              "-1    1933\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No, there is not class imbalance issue. Since, number of samples for class 1 and -1 don't differ so much.**"
      ],
      "metadata": {
        "id": "Rqr0Ogrx5f2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(c)}$"
      ],
      "metadata": {
        "id": "LnaY982T5sWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XKk4eXIjvz46"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train, y_test = train_test_split(X_new,y ,random_state=104, test_size=0.20, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VKFnre9-yowr"
      },
      "outputs": [],
      "source": [
        "train_data = pd.DataFrame(X_train.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4lDJKv_r06z6"
      },
      "outputs": [],
      "source": [
        "train_data.insert(0, 'Labels', y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0y7j15Je3sbn",
        "outputId": "335bd00a-9c10-4a1e-b5d8-982d0d41a6b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Labels         0         1         2         3          4         5  \\\n",
              "0         -1 -1.039071  0.009496  0.067575  0.689276  -0.428018  0.409418   \n",
              "1          1 -1.783699  0.152384 -0.003021  0.310478   0.001870 -0.243077   \n",
              "2          1 -2.298032  0.693171  0.316903  0.525067  -0.124575 -0.316613   \n",
              "3          1 -0.118541 -0.885698  0.014662  0.354838  -0.208659 -0.068768   \n",
              "4         -1 -2.338033  0.315084 -1.086959 -1.280238   9.976684  2.205414   \n",
              "...      ...       ...       ...       ...       ...        ...       ...   \n",
              "3309       1 -3.084614  1.602065  0.073332  0.569104  -0.300533 -0.775108   \n",
              "3310       1  7.300280 -6.042842 -4.936389 -4.704151  -0.809864  0.317562   \n",
              "3311       1  1.218726 -0.392534  3.061458  0.649521  -0.842342  1.507682   \n",
              "3312      -1 -2.329459  0.307269 -1.059447 -1.311362  10.081118  2.251876   \n",
              "3313       1 -0.492756 -0.790298 -1.574949  0.874624  -0.930768 -0.519644   \n",
              "\n",
              "             6         7         8  ...       510       511       512  \\\n",
              "0     0.212412 -0.043705 -0.132766  ...  0.320172 -0.280763 -0.226095   \n",
              "1     0.362368  0.216989 -0.227199  ... -0.040482  0.024195  0.101638   \n",
              "2    -0.262199 -0.074170  0.125734  ... -0.038962 -0.076381 -0.042632   \n",
              "3    -0.194481  0.678728  0.831914  ... -0.289166 -0.272543 -0.184063   \n",
              "4     0.310788 -0.763790 -0.557822  ...  0.011762 -0.024332 -0.101614   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "3309 -0.213606  0.434235 -0.689919  ...  0.005640  0.013835 -0.023102   \n",
              "3310 -3.335242 -0.284289  0.937901  ... -0.023726 -0.046528 -0.185711   \n",
              "3311 -0.037563 -2.178591  1.595719  ... -0.022117  0.063028  0.023287   \n",
              "3312  0.356256 -0.838361 -0.530938  ...  0.022253  0.016871  0.140556   \n",
              "3313 -0.422785  0.867821  0.154920  ... -0.444463 -0.191272 -0.230166   \n",
              "\n",
              "           513       514       515       516       517       518       519  \n",
              "0    -0.152997 -0.260018 -0.221543 -0.114043 -0.144748 -0.132854 -0.063064  \n",
              "1     0.033153 -0.011314  0.290847  0.169626 -0.114721  0.016304 -0.158889  \n",
              "2     0.190299 -0.069709 -0.112635 -0.027490 -0.017056  0.090063 -0.014293  \n",
              "3     0.039644  0.132462 -0.004139 -0.323750 -0.237291 -0.024558 -0.162689  \n",
              "4     0.018798 -0.114098  0.072583  0.079041  0.073613 -0.066852 -0.063503  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "3309  0.004181  0.024730  0.033632  0.009606 -0.000594  0.000232  0.009908  \n",
              "3310 -0.020810  0.256027  0.018287  0.129555 -0.098553 -0.280828  0.030132  \n",
              "3311 -0.007970 -0.012067  0.082583  0.024956  0.010401 -0.089572  0.059250  \n",
              "3312 -0.052450  0.148332 -0.080015 -0.091612 -0.061432  0.080013  0.090590  \n",
              "3313 -0.296990 -0.212068  0.346597  0.165423  0.453525 -0.393605 -0.033957  \n",
              "\n",
              "[3314 rows x 521 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2ada8e83-6c01-4041-b406-dff94d0054f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.039071</td>\n",
              "      <td>0.009496</td>\n",
              "      <td>0.067575</td>\n",
              "      <td>0.689276</td>\n",
              "      <td>-0.428018</td>\n",
              "      <td>0.409418</td>\n",
              "      <td>0.212412</td>\n",
              "      <td>-0.043705</td>\n",
              "      <td>-0.132766</td>\n",
              "      <td>...</td>\n",
              "      <td>0.320172</td>\n",
              "      <td>-0.280763</td>\n",
              "      <td>-0.226095</td>\n",
              "      <td>-0.152997</td>\n",
              "      <td>-0.260018</td>\n",
              "      <td>-0.221543</td>\n",
              "      <td>-0.114043</td>\n",
              "      <td>-0.144748</td>\n",
              "      <td>-0.132854</td>\n",
              "      <td>-0.063064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.783699</td>\n",
              "      <td>0.152384</td>\n",
              "      <td>-0.003021</td>\n",
              "      <td>0.310478</td>\n",
              "      <td>0.001870</td>\n",
              "      <td>-0.243077</td>\n",
              "      <td>0.362368</td>\n",
              "      <td>0.216989</td>\n",
              "      <td>-0.227199</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040482</td>\n",
              "      <td>0.024195</td>\n",
              "      <td>0.101638</td>\n",
              "      <td>0.033153</td>\n",
              "      <td>-0.011314</td>\n",
              "      <td>0.290847</td>\n",
              "      <td>0.169626</td>\n",
              "      <td>-0.114721</td>\n",
              "      <td>0.016304</td>\n",
              "      <td>-0.158889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-2.298032</td>\n",
              "      <td>0.693171</td>\n",
              "      <td>0.316903</td>\n",
              "      <td>0.525067</td>\n",
              "      <td>-0.124575</td>\n",
              "      <td>-0.316613</td>\n",
              "      <td>-0.262199</td>\n",
              "      <td>-0.074170</td>\n",
              "      <td>0.125734</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038962</td>\n",
              "      <td>-0.076381</td>\n",
              "      <td>-0.042632</td>\n",
              "      <td>0.190299</td>\n",
              "      <td>-0.069709</td>\n",
              "      <td>-0.112635</td>\n",
              "      <td>-0.027490</td>\n",
              "      <td>-0.017056</td>\n",
              "      <td>0.090063</td>\n",
              "      <td>-0.014293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.118541</td>\n",
              "      <td>-0.885698</td>\n",
              "      <td>0.014662</td>\n",
              "      <td>0.354838</td>\n",
              "      <td>-0.208659</td>\n",
              "      <td>-0.068768</td>\n",
              "      <td>-0.194481</td>\n",
              "      <td>0.678728</td>\n",
              "      <td>0.831914</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.289166</td>\n",
              "      <td>-0.272543</td>\n",
              "      <td>-0.184063</td>\n",
              "      <td>0.039644</td>\n",
              "      <td>0.132462</td>\n",
              "      <td>-0.004139</td>\n",
              "      <td>-0.323750</td>\n",
              "      <td>-0.237291</td>\n",
              "      <td>-0.024558</td>\n",
              "      <td>-0.162689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>-2.338033</td>\n",
              "      <td>0.315084</td>\n",
              "      <td>-1.086959</td>\n",
              "      <td>-1.280238</td>\n",
              "      <td>9.976684</td>\n",
              "      <td>2.205414</td>\n",
              "      <td>0.310788</td>\n",
              "      <td>-0.763790</td>\n",
              "      <td>-0.557822</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011762</td>\n",
              "      <td>-0.024332</td>\n",
              "      <td>-0.101614</td>\n",
              "      <td>0.018798</td>\n",
              "      <td>-0.114098</td>\n",
              "      <td>0.072583</td>\n",
              "      <td>0.079041</td>\n",
              "      <td>0.073613</td>\n",
              "      <td>-0.066852</td>\n",
              "      <td>-0.063503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3309</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.084614</td>\n",
              "      <td>1.602065</td>\n",
              "      <td>0.073332</td>\n",
              "      <td>0.569104</td>\n",
              "      <td>-0.300533</td>\n",
              "      <td>-0.775108</td>\n",
              "      <td>-0.213606</td>\n",
              "      <td>0.434235</td>\n",
              "      <td>-0.689919</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005640</td>\n",
              "      <td>0.013835</td>\n",
              "      <td>-0.023102</td>\n",
              "      <td>0.004181</td>\n",
              "      <td>0.024730</td>\n",
              "      <td>0.033632</td>\n",
              "      <td>0.009606</td>\n",
              "      <td>-0.000594</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.009908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3310</th>\n",
              "      <td>1</td>\n",
              "      <td>7.300280</td>\n",
              "      <td>-6.042842</td>\n",
              "      <td>-4.936389</td>\n",
              "      <td>-4.704151</td>\n",
              "      <td>-0.809864</td>\n",
              "      <td>0.317562</td>\n",
              "      <td>-3.335242</td>\n",
              "      <td>-0.284289</td>\n",
              "      <td>0.937901</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023726</td>\n",
              "      <td>-0.046528</td>\n",
              "      <td>-0.185711</td>\n",
              "      <td>-0.020810</td>\n",
              "      <td>0.256027</td>\n",
              "      <td>0.018287</td>\n",
              "      <td>0.129555</td>\n",
              "      <td>-0.098553</td>\n",
              "      <td>-0.280828</td>\n",
              "      <td>0.030132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3311</th>\n",
              "      <td>1</td>\n",
              "      <td>1.218726</td>\n",
              "      <td>-0.392534</td>\n",
              "      <td>3.061458</td>\n",
              "      <td>0.649521</td>\n",
              "      <td>-0.842342</td>\n",
              "      <td>1.507682</td>\n",
              "      <td>-0.037563</td>\n",
              "      <td>-2.178591</td>\n",
              "      <td>1.595719</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.022117</td>\n",
              "      <td>0.063028</td>\n",
              "      <td>0.023287</td>\n",
              "      <td>-0.007970</td>\n",
              "      <td>-0.012067</td>\n",
              "      <td>0.082583</td>\n",
              "      <td>0.024956</td>\n",
              "      <td>0.010401</td>\n",
              "      <td>-0.089572</td>\n",
              "      <td>0.059250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3312</th>\n",
              "      <td>-1</td>\n",
              "      <td>-2.329459</td>\n",
              "      <td>0.307269</td>\n",
              "      <td>-1.059447</td>\n",
              "      <td>-1.311362</td>\n",
              "      <td>10.081118</td>\n",
              "      <td>2.251876</td>\n",
              "      <td>0.356256</td>\n",
              "      <td>-0.838361</td>\n",
              "      <td>-0.530938</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022253</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.140556</td>\n",
              "      <td>-0.052450</td>\n",
              "      <td>0.148332</td>\n",
              "      <td>-0.080015</td>\n",
              "      <td>-0.091612</td>\n",
              "      <td>-0.061432</td>\n",
              "      <td>0.080013</td>\n",
              "      <td>0.090590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3313</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.492756</td>\n",
              "      <td>-0.790298</td>\n",
              "      <td>-1.574949</td>\n",
              "      <td>0.874624</td>\n",
              "      <td>-0.930768</td>\n",
              "      <td>-0.519644</td>\n",
              "      <td>-0.422785</td>\n",
              "      <td>0.867821</td>\n",
              "      <td>0.154920</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.444463</td>\n",
              "      <td>-0.191272</td>\n",
              "      <td>-0.230166</td>\n",
              "      <td>-0.296990</td>\n",
              "      <td>-0.212068</td>\n",
              "      <td>0.346597</td>\n",
              "      <td>0.165423</td>\n",
              "      <td>0.453525</td>\n",
              "      <td>-0.393605</td>\n",
              "      <td>-0.033957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3314 rows × 521 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ada8e83-6c01-4041-b406-dff94d0054f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2ada8e83-6c01-4041-b406-dff94d0054f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2ada8e83-6c01-4041-b406-dff94d0054f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WxpLwwfP3wOx",
        "outputId": "f8bd7a89-1f84-4bcd-b0b3-d9e268ea0d8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Labels          0         1         2         3         4         5  \\\n",
              "0        -1  -0.059278 -0.524923 -0.381460 -0.429855  0.576556  0.233927   \n",
              "1        -1  -0.952151 -0.295575 -1.541324 -1.111016 -0.090549 -0.289403   \n",
              "2        -1  -1.894287  0.714721  0.596764  0.755272 -0.535002  0.039263   \n",
              "3         1  11.811218 -8.512016  9.051152 -1.469777  2.481459 -5.332863   \n",
              "4        -1  -2.335408  0.747366  0.452392  0.342893 -0.359686 -0.071687   \n",
              "..      ...        ...       ...       ...       ...       ...       ...   \n",
              "824      -1   2.471277 -1.646650 -2.921166 -4.128645 -1.167553  2.352743   \n",
              "825       1  -3.138283  1.505443  0.074280  0.558178 -0.306863 -0.764891   \n",
              "826      -1  -3.155592  1.318856  0.016208  0.525506 -0.344629 -0.610560   \n",
              "827      -1  -1.509192 -0.105267 -0.947249 -0.152706 -0.367935 -0.327515   \n",
              "828       1  -0.828253  0.113349  0.779977  0.896773 -0.130299  0.277717   \n",
              "\n",
              "            6         7         8  ...       510       511       512  \\\n",
              "0    0.474237  0.717917  0.345152  ... -0.010222  0.192978  0.188560   \n",
              "1   -0.113607  0.291050  0.506458  ... -0.116827 -0.046791 -0.495561   \n",
              "2    0.086175 -0.023623 -0.017627  ...  0.173324 -0.178970  0.029844   \n",
              "3   -2.997072  0.834283 -1.687244  ... -0.184900  0.114462 -0.010357   \n",
              "4    0.214835  0.134342 -0.259464  ...  0.276105 -0.061621 -0.244448   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "824 -2.199456 -4.345573 -1.197475  ...  0.049071  0.071923 -0.014776   \n",
              "825 -0.216171  0.422470 -0.683813  ... -0.012135  0.034605 -0.039965   \n",
              "826 -0.233485  0.193882 -0.843261  ...  0.089734  0.035664  0.061349   \n",
              "827 -0.253428  0.920542 -0.337864  ... -0.092496  0.024295 -0.009437   \n",
              "828  0.616525  0.007581  0.800523  ... -0.001833  0.068963  0.081447   \n",
              "\n",
              "          513       514       515       516       517       518       519  \n",
              "0    0.035730  0.086566  0.092732 -0.168813  0.114429 -0.415690  0.012432  \n",
              "1   -0.058068 -0.162131 -0.132096  0.241108 -0.586222  0.340069 -0.046269  \n",
              "2   -0.043502  0.114961 -0.431774 -0.044987 -0.015400 -0.070218  0.065966  \n",
              "3   -0.239816 -0.097504 -0.049876 -0.624349  1.117170 -0.161854  0.175257  \n",
              "4   -0.042089 -0.097203  0.039878 -0.255603 -0.215578  0.381308  0.310693  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "824 -0.016007  0.011275  0.128003  0.119203 -0.007158 -0.236152 -0.202326  \n",
              "825 -0.015127  0.031285  0.028599 -0.008398 -0.004972 -0.031710  0.015532  \n",
              "826 -0.006594  0.078205  0.043207  0.043982  0.093089  0.027115 -0.093106  \n",
              "827  0.000859  0.000037 -0.106850  0.065515  0.031912  0.050987  0.062513  \n",
              "828 -0.089717  0.454525  0.549359 -0.440261 -0.158940  0.233387  0.168716  \n",
              "\n",
              "[829 rows x 521 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b113951-f100-453c-864b-3af1a5656d8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>-0.059278</td>\n",
              "      <td>-0.524923</td>\n",
              "      <td>-0.381460</td>\n",
              "      <td>-0.429855</td>\n",
              "      <td>0.576556</td>\n",
              "      <td>0.233927</td>\n",
              "      <td>0.474237</td>\n",
              "      <td>0.717917</td>\n",
              "      <td>0.345152</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.010222</td>\n",
              "      <td>0.192978</td>\n",
              "      <td>0.188560</td>\n",
              "      <td>0.035730</td>\n",
              "      <td>0.086566</td>\n",
              "      <td>0.092732</td>\n",
              "      <td>-0.168813</td>\n",
              "      <td>0.114429</td>\n",
              "      <td>-0.415690</td>\n",
              "      <td>0.012432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>-0.952151</td>\n",
              "      <td>-0.295575</td>\n",
              "      <td>-1.541324</td>\n",
              "      <td>-1.111016</td>\n",
              "      <td>-0.090549</td>\n",
              "      <td>-0.289403</td>\n",
              "      <td>-0.113607</td>\n",
              "      <td>0.291050</td>\n",
              "      <td>0.506458</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.116827</td>\n",
              "      <td>-0.046791</td>\n",
              "      <td>-0.495561</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.162131</td>\n",
              "      <td>-0.132096</td>\n",
              "      <td>0.241108</td>\n",
              "      <td>-0.586222</td>\n",
              "      <td>0.340069</td>\n",
              "      <td>-0.046269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.894287</td>\n",
              "      <td>0.714721</td>\n",
              "      <td>0.596764</td>\n",
              "      <td>0.755272</td>\n",
              "      <td>-0.535002</td>\n",
              "      <td>0.039263</td>\n",
              "      <td>0.086175</td>\n",
              "      <td>-0.023623</td>\n",
              "      <td>-0.017627</td>\n",
              "      <td>...</td>\n",
              "      <td>0.173324</td>\n",
              "      <td>-0.178970</td>\n",
              "      <td>0.029844</td>\n",
              "      <td>-0.043502</td>\n",
              "      <td>0.114961</td>\n",
              "      <td>-0.431774</td>\n",
              "      <td>-0.044987</td>\n",
              "      <td>-0.015400</td>\n",
              "      <td>-0.070218</td>\n",
              "      <td>0.065966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>11.811218</td>\n",
              "      <td>-8.512016</td>\n",
              "      <td>9.051152</td>\n",
              "      <td>-1.469777</td>\n",
              "      <td>2.481459</td>\n",
              "      <td>-5.332863</td>\n",
              "      <td>-2.997072</td>\n",
              "      <td>0.834283</td>\n",
              "      <td>-1.687244</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.184900</td>\n",
              "      <td>0.114462</td>\n",
              "      <td>-0.010357</td>\n",
              "      <td>-0.239816</td>\n",
              "      <td>-0.097504</td>\n",
              "      <td>-0.049876</td>\n",
              "      <td>-0.624349</td>\n",
              "      <td>1.117170</td>\n",
              "      <td>-0.161854</td>\n",
              "      <td>0.175257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>-2.335408</td>\n",
              "      <td>0.747366</td>\n",
              "      <td>0.452392</td>\n",
              "      <td>0.342893</td>\n",
              "      <td>-0.359686</td>\n",
              "      <td>-0.071687</td>\n",
              "      <td>0.214835</td>\n",
              "      <td>0.134342</td>\n",
              "      <td>-0.259464</td>\n",
              "      <td>...</td>\n",
              "      <td>0.276105</td>\n",
              "      <td>-0.061621</td>\n",
              "      <td>-0.244448</td>\n",
              "      <td>-0.042089</td>\n",
              "      <td>-0.097203</td>\n",
              "      <td>0.039878</td>\n",
              "      <td>-0.255603</td>\n",
              "      <td>-0.215578</td>\n",
              "      <td>0.381308</td>\n",
              "      <td>0.310693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>-1</td>\n",
              "      <td>2.471277</td>\n",
              "      <td>-1.646650</td>\n",
              "      <td>-2.921166</td>\n",
              "      <td>-4.128645</td>\n",
              "      <td>-1.167553</td>\n",
              "      <td>2.352743</td>\n",
              "      <td>-2.199456</td>\n",
              "      <td>-4.345573</td>\n",
              "      <td>-1.197475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049071</td>\n",
              "      <td>0.071923</td>\n",
              "      <td>-0.014776</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>0.011275</td>\n",
              "      <td>0.128003</td>\n",
              "      <td>0.119203</td>\n",
              "      <td>-0.007158</td>\n",
              "      <td>-0.236152</td>\n",
              "      <td>-0.202326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>1</td>\n",
              "      <td>-3.138283</td>\n",
              "      <td>1.505443</td>\n",
              "      <td>0.074280</td>\n",
              "      <td>0.558178</td>\n",
              "      <td>-0.306863</td>\n",
              "      <td>-0.764891</td>\n",
              "      <td>-0.216171</td>\n",
              "      <td>0.422470</td>\n",
              "      <td>-0.683813</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012135</td>\n",
              "      <td>0.034605</td>\n",
              "      <td>-0.039965</td>\n",
              "      <td>-0.015127</td>\n",
              "      <td>0.031285</td>\n",
              "      <td>0.028599</td>\n",
              "      <td>-0.008398</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.031710</td>\n",
              "      <td>0.015532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>-1</td>\n",
              "      <td>-3.155592</td>\n",
              "      <td>1.318856</td>\n",
              "      <td>0.016208</td>\n",
              "      <td>0.525506</td>\n",
              "      <td>-0.344629</td>\n",
              "      <td>-0.610560</td>\n",
              "      <td>-0.233485</td>\n",
              "      <td>0.193882</td>\n",
              "      <td>-0.843261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.089734</td>\n",
              "      <td>0.035664</td>\n",
              "      <td>0.061349</td>\n",
              "      <td>-0.006594</td>\n",
              "      <td>0.078205</td>\n",
              "      <td>0.043207</td>\n",
              "      <td>0.043982</td>\n",
              "      <td>0.093089</td>\n",
              "      <td>0.027115</td>\n",
              "      <td>-0.093106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1.509192</td>\n",
              "      <td>-0.105267</td>\n",
              "      <td>-0.947249</td>\n",
              "      <td>-0.152706</td>\n",
              "      <td>-0.367935</td>\n",
              "      <td>-0.327515</td>\n",
              "      <td>-0.253428</td>\n",
              "      <td>0.920542</td>\n",
              "      <td>-0.337864</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.092496</td>\n",
              "      <td>0.024295</td>\n",
              "      <td>-0.009437</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.106850</td>\n",
              "      <td>0.065515</td>\n",
              "      <td>0.031912</td>\n",
              "      <td>0.050987</td>\n",
              "      <td>0.062513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.828253</td>\n",
              "      <td>0.113349</td>\n",
              "      <td>0.779977</td>\n",
              "      <td>0.896773</td>\n",
              "      <td>-0.130299</td>\n",
              "      <td>0.277717</td>\n",
              "      <td>0.616525</td>\n",
              "      <td>0.007581</td>\n",
              "      <td>0.800523</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001833</td>\n",
              "      <td>0.068963</td>\n",
              "      <td>0.081447</td>\n",
              "      <td>-0.089717</td>\n",
              "      <td>0.454525</td>\n",
              "      <td>0.549359</td>\n",
              "      <td>-0.440261</td>\n",
              "      <td>-0.158940</td>\n",
              "      <td>0.233387</td>\n",
              "      <td>0.168716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>829 rows × 521 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b113951-f100-453c-864b-3af1a5656d8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5b113951-f100-453c-864b-3af1a5656d8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5b113951-f100-453c-864b-3af1a5656d8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "test_data = pd.DataFrame(X_test.copy())\n",
        "test_data.insert(0, 'Labels', y_test)\n",
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jXKQ18X4JE-",
        "outputId": "77c4f56b-e891-4314-9ab3-e0a2286f4ae0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    1788\n",
              "-1    1526\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train_data['Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mugxwkhr4cuL",
        "outputId": "631eecd0-77c6-4189-9a5a-833ce5b08807"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    422\n",
              "-1    407\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "test_data['Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is no class imbalance issue in train and test data also i.e., they are having similar class proportions.**"
      ],
      "metadata": {
        "id": "w9HkUPdD5whM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d)}$"
      ],
      "metadata": {
        "id": "enZq4RYY6FWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ew84LPuJL1Oo"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge,RidgeCV\n",
        "from sklearn.linear_model import Lasso,LassoCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import validation_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vUqAEObUoFPD"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(i))}$"
      ],
      "metadata": {
        "id": "c32bLJmc6WAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2R-0wmLiaehQ"
      },
      "outputs": [],
      "source": [
        "param_range = [1e-5,1e-4,1e-3,1e-2,1e-1,1,10,100,1000,10000,1e5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OukE_FgGLJOv",
        "outputId": "81449c21-0dee-4947-af23-27cefb207502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[0.5397963  0.53941909 0.53941909 0.53941909 0.53959276]\n",
            " [0.75330064 0.7453791  0.74915126 0.74500189 0.75527903]\n",
            " [0.8434553  0.83213882 0.83591098 0.83553376 0.83220211]\n",
            " [0.89701999 0.89626556 0.90305545 0.90041494 0.90196078]\n",
            " [0.93323274 0.93247831 0.93511882 0.93587326 0.9392911 ]\n",
            " [0.95699736 0.95548849 0.96001509 0.95435685 0.9566365 ]\n",
            " [0.97925311 0.97623538 0.97849868 0.97698982 0.97322775]\n",
            " [0.98377971 0.98227084 0.98755187 0.98679743 0.98491704]\n",
            " [0.98755187 0.98377971 0.99094681 0.9905696  0.98717949]\n",
            " [0.98906073 0.98377971 0.99170124 0.99094681 0.98717949]\n",
            " [0.9883063  0.98302527 0.9905696  0.99094681 0.98717949]]\n",
            "val scores: [[0.53846154 0.53996983 0.53996983 0.53996983 0.53927492]\n",
            " [0.70135747 0.73906486 0.78129713 0.7571644  0.74622356]\n",
            " [0.78431373 0.826546   0.84917044 0.8280543  0.82628399]\n",
            " [0.84766214 0.88386124 0.88235294 0.87028658 0.86404834]\n",
            " [0.87933635 0.88687783 0.88536953 0.89140271 0.8776435 ]\n",
            " [0.86726998 0.88084465 0.85972851 0.87330317 0.87311178]\n",
            " [0.85822021 0.8627451  0.85822021 0.88084465 0.87009063]\n",
            " [0.85972851 0.86425339 0.8612368  0.87933635 0.87613293]\n",
            " [0.8627451  0.85671192 0.85822021 0.88235294 0.87915408]\n",
            " [0.8627451  0.85822021 0.85671192 0.88536953 0.8776435 ]\n",
            " [0.8612368  0.8612368  0.85671192 0.87631976 0.87915408]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 1e-05\n",
            "train scores: [0.5397963  0.53941909 0.53941909 0.53941909 0.53959276]\n",
            "val scores: [0.53846154 0.53996983 0.53996983 0.53996983 0.53927492]\n",
            "**************************\n",
            "alpha: 0.0001\n",
            "train scores: [0.75330064 0.7453791  0.74915126 0.74500189 0.75527903]\n",
            "val scores: [0.70135747 0.73906486 0.78129713 0.7571644  0.74622356]\n",
            "**************************\n",
            "alpha: 0.001\n",
            "train scores: [0.8434553  0.83213882 0.83591098 0.83553376 0.83220211]\n",
            "val scores: [0.78431373 0.826546   0.84917044 0.8280543  0.82628399]\n",
            "**************************\n",
            "alpha: 0.01\n",
            "train scores: [0.89701999 0.89626556 0.90305545 0.90041494 0.90196078]\n",
            "val scores: [0.84766214 0.88386124 0.88235294 0.87028658 0.86404834]\n",
            "**************************\n",
            "alpha: 0.1\n",
            "train scores: [0.93323274 0.93247831 0.93511882 0.93587326 0.9392911 ]\n",
            "val scores: [0.87933635 0.88687783 0.88536953 0.89140271 0.8776435 ]\n",
            "**************************\n",
            "alpha: 1\n",
            "train scores: [0.95699736 0.95548849 0.96001509 0.95435685 0.9566365 ]\n",
            "val scores: [0.86726998 0.88084465 0.85972851 0.87330317 0.87311178]\n",
            "**************************\n",
            "alpha: 10\n",
            "train scores: [0.97925311 0.97623538 0.97849868 0.97698982 0.97322775]\n",
            "val scores: [0.85822021 0.8627451  0.85822021 0.88084465 0.87009063]\n",
            "**************************\n",
            "alpha: 100\n",
            "train scores: [0.98377971 0.98227084 0.98755187 0.98679743 0.98491704]\n",
            "val scores: [0.85972851 0.86425339 0.8612368  0.87933635 0.87613293]\n",
            "**************************\n",
            "alpha: 1000\n",
            "train scores: [0.98755187 0.98377971 0.99094681 0.9905696  0.98717949]\n",
            "val scores: [0.8627451  0.85671192 0.85822021 0.88235294 0.87915408]\n",
            "**************************\n",
            "alpha: 10000\n",
            "train scores: [0.98906073 0.98377971 0.99170124 0.99094681 0.98717949]\n",
            "val scores: [0.8627451  0.85822021 0.85671192 0.88536953 0.8776435 ]\n",
            "**************************\n",
            "alpha: 100000.0\n",
            "train scores: [0.9883063  0.98302527 0.9905696  0.99094681 0.98717949]\n",
            "val scores: [0.8612368  0.8612368  0.85671192 0.87631976 0.87915408]\n",
            "**************************\n"
          ]
        }
      ],
      "source": [
        "pipeline = make_pipeline(LogisticRegression(solver='lbfgs', penalty='l2', random_state=1))\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline, X=X_train, y=y_train, cv=5, param_name='logisticregression__C', param_range=param_range)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param_range)):\n",
        "  print('alpha:', param_range[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "b0QO5FUf6Hgw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EmW126WS5-YY"
      },
      "outputs": [],
      "source": [
        "accuracy_train=[]\n",
        "recall_train=[]\n",
        "precision_train=[]\n",
        "f1_score_train=[]\n",
        "specifity_train=[]\n",
        "senstivity_train=[]\n",
        "accuracy_test=[]\n",
        "recall_test=[]\n",
        "precision_test=[]\n",
        "f1_score_test=[]\n",
        "specifity_test=[]\n",
        "senstivity_test=[]\n",
        "best_hyper_parameter=[]\n",
        "def metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test):\n",
        "  accuracy_train.append(accuracy_score(y_train,y_pred_train))\n",
        "  recall_train.append(recall_score(y_train,y_pred_train))\n",
        "  precision_train.append(precision_score(y_train,y_pred_train))\n",
        "  f1_score_train.append(f1_score(y_train,y_pred_train))\n",
        "  tn, fp, fn, tp = confusion_matrix(y_train, y_pred_train).ravel()\n",
        "  specificity1 = tn / (tn+fp)\n",
        "  senstivity1= tp / (tp + fn)\n",
        "  specifity_train.append(specificity1)\n",
        "  senstivity_train.append(senstivity1)\n",
        "  accuracy_test.append(accuracy_score(y_test,y_pred_test))\n",
        "  recall_test.append(recall_score(y_test,y_pred_test))\n",
        "  precision_test.append(precision_score(y_test,y_pred_test))\n",
        "  f1_score_test.append(f1_score(y_test,y_pred_test))\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "  specificity1 = tn / (tn+fp)\n",
        "  senstivity1= tp / (tp + fn)\n",
        "  specifity_test.append(specificity1)\n",
        "  senstivity_test.append(senstivity1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KpVig3vZYpH",
        "outputId": "593187ed-5b94-4813-fa36-8a09b93ff96c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average train scores : [0.53952926 0.74962239 0.83584819 0.89974335 0.93519885 0.95669886\n",
            " 0.97684095 0.98506338 0.98800549 0.9885336  0.98800549]\n",
            "average val scores : [0.53952919 0.74502149 0.82287369 0.86964225 0.88412599 0.87085162\n",
            " 0.86602416 0.8681376  0.86783685 0.86813805 0.86693187]\n"
          ]
        }
      ],
      "source": [
        "#print(train_scores.shape)\n",
        "avg_train_scores = np.mean(train_scores,axis=1)\n",
        "avg_val_scores = np.mean(val_scores,axis=1)\n",
        "print('average train scores :',avg_train_scores)\n",
        "print('average val scores :',avg_val_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6DgpjapZn5B",
        "outputId": "31540cbd-a6d0-42aa-bfcf-216d153164a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best alpha from 5 fold CV: 0.1\n"
          ]
        }
      ],
      "source": [
        "#best alpha\n",
        "\n",
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best alpha from 5 fold CV:',best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4bnvCI6A_XCe",
        "outputId": "a810b6de-c38c-4b8e-80fa-d00bc05b0af9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, random_state=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, random_state=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "clf = LogisticRegression(solver='lbfgs', penalty='l2', C = best_alpha, random_state=1)\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGSaoUfM_Ycq",
        "outputId": "16e3f2d2-75a9-4875-e7f5-cbaac0d32a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.38611527  0.42449703  0.87258799  0.8499544  -0.39803513  0.33051075\n",
            "  -0.00920709  0.43635468 -0.08709481 -0.73244412 -0.29213645 -0.36988026\n",
            "   1.05602559  0.03983041 -0.11836814 -0.27957495 -0.07205944 -0.3145592\n",
            "  -0.1164937   0.58150771 -0.0892014  -0.13622313  0.00617414  0.07363905\n",
            "   0.25489573  0.17305378 -0.16509012 -0.43202701 -0.08411671  0.16569824\n",
            "  -0.16209135 -0.05067727 -0.35732681 -0.33478028  0.12104278 -0.04484708\n",
            "   0.50683375 -0.35617284 -0.12495716  0.14418821 -0.02725553 -0.11571081\n",
            "  -0.13434008 -0.56923299 -0.29313347 -0.01176833  0.16176277  0.36815003\n",
            "  -0.12631232  0.08221771  0.06598408  0.42966795  0.26788056 -0.05066988\n",
            "  -0.15925258 -0.15777657  0.0592614  -0.08691383 -0.17810572  0.25645892\n",
            "  -0.20153491  0.2178147   0.03697238  0.36266681  0.03873862 -0.50413918\n",
            "  -0.06344263  0.2997841   0.04468871  0.52228421  0.39040998  0.06778493\n",
            "  -0.02789476  0.310675    0.31583807 -0.44341371 -0.27619357 -0.11513846\n",
            "  -0.27498844 -0.00282799  0.56321381  0.19671461  0.07213012  0.33287014\n",
            "   0.41062204  0.20195501 -0.17669536  0.44486764 -0.33129662  0.07446103\n",
            "   0.45613383 -0.39633631  0.11243593 -0.08068088  0.12209437  0.23995586\n",
            "  -0.11580766 -0.1755033  -0.1477236  -0.54149395 -0.08622034 -0.02041424\n",
            "   0.33571009  0.11144373  0.08474801  0.24220856  0.01507955 -0.19738943\n",
            "  -0.19958735 -0.22679787 -0.56842337  0.09184882  0.16643304  0.22311027\n",
            "  -0.01742074  0.36759109  0.08713441  0.22603587  0.13118794 -0.28269733\n",
            "  -0.1845802   0.02835427  0.11627298  0.26513489 -0.16169073 -0.06954108\n",
            "  -0.03025419  0.15199128 -0.14176366  0.2771994   0.48444115  0.03177211\n",
            "  -0.06243681 -0.08230559 -0.12262372  0.04477303  0.22824862 -0.06315695\n",
            "  -0.31606775 -0.23160857 -0.04678855 -0.14796503 -0.02788908 -0.09924882\n",
            "  -0.44282779 -0.01623085 -0.037842   -0.17725276  0.06113524  0.09972592\n",
            "   0.3840055   0.20635306 -0.14251493  0.22715104 -0.35587027 -0.13616401\n",
            "   0.28221354  0.04873044  0.04568624 -0.15319095  0.03667315 -0.38864542\n",
            "  -0.10832519 -0.15775583 -0.14064128  0.06752281  0.26943025 -0.24670144\n",
            "   0.01090085 -0.32485868 -0.27163101 -0.18467224 -0.39939826 -0.10572876\n",
            "   0.27511946 -0.15736785  0.13076581  0.1759554   0.16720698  0.13972544\n",
            "  -0.25359671  0.1453975  -0.37035678 -0.11038115 -0.06438757  0.03891301\n",
            "  -0.09708911 -0.12567135 -0.05362228  0.05659959  0.22370713 -0.29603192\n",
            "   0.12236087 -0.22858726  0.18858395  0.02884483  0.42816144 -0.29148799\n",
            "   0.07542351  0.20795352  0.04070637 -0.22341133 -0.19375992  0.14746024\n",
            "  -0.23552293 -0.10691252 -0.08670881 -0.01627936 -0.1931583   0.14710428\n",
            "  -0.25480547 -0.32924295  0.4508247   0.31735811  0.25452662  0.30538924\n",
            "  -0.24819548  0.00643498  0.26798112  0.08240751 -0.03580619 -0.22091011\n",
            "  -0.11033791  0.14735147  0.09841462 -0.36075519  0.55472885  0.08217906\n",
            "  -0.23239749  0.06307085 -0.07716897  0.22463568 -0.20460533 -0.15231725\n",
            "   0.03530612  0.17950203 -0.23572697 -0.19488635 -0.23380189 -0.17537281\n",
            "   0.16674915  0.18926447 -0.06319684 -0.30330117 -0.13937522  0.48772943\n",
            "  -0.09531891 -0.18649166 -0.23044781 -0.00549843 -0.06441078 -0.03643303\n",
            "  -0.21694973  0.01043501  0.26302082  0.10635045 -0.44516523 -0.16252186\n",
            "   0.34301423  0.01700941  0.19859106 -0.27122166  0.07293024  0.07739961\n",
            "   0.29479644 -0.5047884  -0.18840438 -0.33587707  0.2096539  -0.04238923\n",
            "   0.16467012  0.20623532 -0.15367933 -0.00873705 -0.10363265 -0.35587732\n",
            "  -0.03969781  0.04731449  0.10661447  0.0574942  -0.39937748  0.14297696\n",
            "   0.18944024 -0.42398367 -0.14584962 -0.25851768 -0.2181999  -0.04136013\n",
            "   0.1024121   0.12494622 -0.05712751 -0.02924132 -0.27119981 -0.18414504\n",
            "   0.20827363 -0.09052891 -0.25441145 -0.19116949  0.30508063 -0.32046522\n",
            "   0.14601909 -0.2814552  -0.16992955 -0.04896918  0.14240113  0.23883618\n",
            "   0.16839985 -0.10012801  0.27927504  0.43265661  0.09923274  0.60822778\n",
            "  -0.02444484  0.05341808  0.39959543  0.13184501 -0.18586111  0.02994883\n",
            "  -0.12391878  0.15487514  0.19218954 -0.08355607 -0.02035642 -0.18791779\n",
            "   0.04418734  0.39107276 -0.06865992 -0.06798538 -0.05582802 -0.05382466\n",
            "  -0.20504784 -0.10206832  0.02529541  0.15251455  0.03653233  0.13527704\n",
            "   0.06349114 -0.17872965 -0.14694567  0.44257497 -0.00164832  0.19061957\n",
            "  -0.07646709 -0.02354597  0.05610505 -0.00119169  0.20813109 -0.01414529\n",
            "   0.39185918 -0.12153076  0.27251839 -0.45041175  0.29597176 -0.02556099\n",
            "  -0.41122183 -0.21048682  0.09425739 -0.17796814 -0.07039942  0.17576135\n",
            "   0.03639135 -0.06615268 -0.02083966 -0.04359673 -0.22975191 -0.30140369\n",
            "  -0.18392478 -0.03470486  0.48233791  0.24969155  0.12462045  0.09169201\n",
            "  -0.55717274 -0.03988859  0.10903744  0.26142953 -0.21232213  0.21916488\n",
            "   0.06353687 -0.05123511  0.28695761 -0.03602143 -0.03714421  0.09109439\n",
            "   0.16895982 -0.00255044  0.38973201 -0.14453617 -0.2219873  -0.04327588\n",
            "  -0.17213087 -0.12219707  0.35196906  0.42522049 -0.16244195 -0.14756394\n",
            "  -0.0182185   0.167928    0.10283026 -0.03113104 -0.08984291 -0.04108304\n",
            "  -0.17532241 -0.08737346  0.27276213  0.04654948  0.12448893  0.4107509\n",
            "  -0.27658924  0.06777144  0.09080441  0.01201119  0.13033231 -0.30437508\n",
            "   0.06750046 -0.06246338  0.03136646 -0.07799776  0.33220751 -0.12592746\n",
            "   0.03238046  0.04724734  0.14114938 -0.24867183  0.09335687 -0.1697697\n",
            "   0.06380918  0.07045058 -0.06578236  0.06952928  0.41749231 -0.01312856\n",
            "  -0.51832851  0.07923389  0.33765557 -0.04448297 -0.08944574  0.07055758\n",
            "   0.15785803 -0.00678222  0.08538121 -0.10812047 -0.02695474 -0.11433265\n",
            "   0.14238405 -0.20001672  0.02735084 -0.00168321  0.1011738  -0.0655317\n",
            "   0.1096883   0.00686353 -0.04644356 -0.14998599  0.13967941  0.05752296\n",
            "   0.06000297  0.03654932 -0.02012086  0.28505818  0.08710276  0.01417072\n",
            "  -0.13328546 -0.11296562 -0.10081229 -0.05080086  0.21196221  0.14208955\n",
            "   0.04121872  0.03697109 -0.09649255  0.03704383  0.29400613 -0.10118498\n",
            "  -0.09939297 -0.10171126  0.07755334  0.28242653 -0.10203479 -0.06667998\n",
            "  -0.05220926 -0.05031548  0.34097806 -0.15009277 -0.27640201 -0.27310889\n",
            "  -0.02334621  0.01215355  0.15539736 -0.11710022 -0.10889721  0.01697699\n",
            "   0.12207839  0.53962527  0.27957186  0.23819862 -0.10212514  0.03218836\n",
            "  -0.20151691  0.10011855  0.17994581  0.29343348 -0.15131539  0.21886272\n",
            "   0.2963164  -0.24066509  0.05417635  0.04114503  0.30431317  0.11230992\n",
            "   0.06620357 -0.00898993  0.02575922 -0.40141407 -0.19973941  0.14268343\n",
            "  -0.01530899  0.09105874 -0.20910534  0.04056794]]\n"
          ]
        }
      ],
      "source": [
        "print(clf.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Yjl5nx0xZshY"
      },
      "outputs": [],
      "source": [
        "# class_ = [-1, 1]\n",
        "# for k in class_:\n",
        "#   print(k)\n",
        "#   print('class:',k, 'coefficients:', clf.coef_[k])\n",
        "y_pred_train =clf.predict(X_train)\n",
        "y_pred_test=clf.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygulqnHE_3V8",
        "outputId": "07a8173c-4fe1-479e-d305-c9ff5ac58efa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9330114665057333]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "accuracy_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUwpBL4JAI4s",
        "outputId": "074ca52b-e1a5-4c0a-cda9-927d57eee5bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.887816646562123]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "accuracy_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(iii))}$"
      ],
      "metadata": {
        "id": "o_LzabRO6h1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATjWm7gedI5I",
        "outputId": "12e05244-8f9e-449c-9bc6-377c8e75849d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[0.83968314 0.79932101 0.77744247 0.7849868  0.77941176]\n",
            " [0.8457186  0.84420973 0.85665786 0.8457186  0.85784314]\n",
            " [0.89626556 0.89437948 0.90079215 0.90003772 0.90120664]\n",
            " [0.93549604 0.93134666 0.93662769 0.93587326 0.93853695]\n",
            " [0.95662014 0.95473406 0.96152395 0.95473406 0.95852187]\n",
            " [0.97736703 0.97623538 0.97548095 0.97548095 0.97435897]\n",
            " [0.9649189  0.9626556  0.96831384 0.97925311 0.9754902 ]\n",
            " [0.95624293 0.94417201 0.93813655 0.94379479 0.96153846]\n",
            " [0.94832139 0.95020747 0.92870615 0.94794417 0.9494721 ]\n",
            " [0.94945304 0.95209355 0.92870615 0.94228593 0.94155354]\n",
            " [0.94907582 0.92606564 0.93247831 0.93096945 0.89555053]]\n",
            "val scores: [[0.81900452 0.78431373 0.76319759 0.80542986 0.76586103]\n",
            " [0.79487179 0.84615385 0.86425339 0.82956259 0.84743202]\n",
            " [0.84464555 0.88235294 0.88386124 0.86425339 0.85649547]\n",
            " [0.87631976 0.88386124 0.87782805 0.88386124 0.87462236]\n",
            " [0.8627451  0.87631976 0.86576169 0.87782805 0.8776435 ]\n",
            " [0.85218703 0.8627451  0.85067873 0.88084465 0.86858006]\n",
            " [0.82352941 0.82956259 0.84615385 0.85067873 0.83685801]\n",
            " [0.82352941 0.80542986 0.81598793 0.82202112 0.8141994 ]\n",
            " [0.81598793 0.81749623 0.79939668 0.84766214 0.80513595]\n",
            " [0.80844646 0.81900452 0.80090498 0.82051282 0.79758308]\n",
            " [0.80542986 0.80542986 0.81146305 0.82352941 0.77643505]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 1e-05\n",
            "train scores: [0.83968314 0.79932101 0.77744247 0.7849868  0.77941176]\n",
            "val scores: [0.81900452 0.78431373 0.76319759 0.80542986 0.76586103]\n",
            "**************************\n",
            "alpha: 0.0001\n",
            "train scores: [0.8457186  0.84420973 0.85665786 0.8457186  0.85784314]\n",
            "val scores: [0.79487179 0.84615385 0.86425339 0.82956259 0.84743202]\n",
            "**************************\n",
            "alpha: 0.001\n",
            "train scores: [0.89626556 0.89437948 0.90079215 0.90003772 0.90120664]\n",
            "val scores: [0.84464555 0.88235294 0.88386124 0.86425339 0.85649547]\n",
            "**************************\n",
            "alpha: 0.01\n",
            "train scores: [0.93549604 0.93134666 0.93662769 0.93587326 0.93853695]\n",
            "val scores: [0.87631976 0.88386124 0.87782805 0.88386124 0.87462236]\n",
            "**************************\n",
            "alpha: 0.1\n",
            "train scores: [0.95662014 0.95473406 0.96152395 0.95473406 0.95852187]\n",
            "val scores: [0.8627451  0.87631976 0.86576169 0.87782805 0.8776435 ]\n",
            "**************************\n",
            "alpha: 1\n",
            "train scores: [0.97736703 0.97623538 0.97548095 0.97548095 0.97435897]\n",
            "val scores: [0.85218703 0.8627451  0.85067873 0.88084465 0.86858006]\n",
            "**************************\n",
            "alpha: 10\n",
            "train scores: [0.9649189  0.9626556  0.96831384 0.97925311 0.9754902 ]\n",
            "val scores: [0.82352941 0.82956259 0.84615385 0.85067873 0.83685801]\n",
            "**************************\n",
            "alpha: 100\n",
            "train scores: [0.95624293 0.94417201 0.93813655 0.94379479 0.96153846]\n",
            "val scores: [0.82352941 0.80542986 0.81598793 0.82202112 0.8141994 ]\n",
            "**************************\n",
            "alpha: 1000\n",
            "train scores: [0.94832139 0.95020747 0.92870615 0.94794417 0.9494721 ]\n",
            "val scores: [0.81598793 0.81749623 0.79939668 0.84766214 0.80513595]\n",
            "**************************\n",
            "alpha: 10000\n",
            "train scores: [0.94945304 0.95209355 0.92870615 0.94228593 0.94155354]\n",
            "val scores: [0.80844646 0.81900452 0.80090498 0.82051282 0.79758308]\n",
            "**************************\n",
            "alpha: 100000.0\n",
            "train scores: [0.94907582 0.92606564 0.93247831 0.93096945 0.89555053]\n",
            "val scores: [0.80542986 0.80542986 0.81146305 0.82352941 0.77643505]\n",
            "**************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "pipeline = make_pipeline(LinearSVC(penalty='l2'))\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline, X=X_train, y=y_train, cv=5, param_name='linearsvc__C', param_range=param_range)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param_range)):\n",
        "  print('alpha:', param_range[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NnobCognmpy",
        "outputId": "fc5c5cac-c00d-465b-8fc9-40b477a4bfb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average train scores : [0.79616904 0.85002959 0.89853631 0.93557612 0.95722682 0.97578466\n",
            " 0.97012633 0.94877695 0.94493025 0.94281844 0.92682795]\n",
            "average val scores : [0.78756135 0.83645473 0.86632172 0.87929853 0.87205962 0.86300711\n",
            " 0.83735652 0.81623354 0.81713579 0.80929037 0.80445745]\n"
          ]
        }
      ],
      "source": [
        "#print(train_scores.shape)\n",
        "avg_train_scores = np.mean(train_scores,axis=1)\n",
        "avg_val_scores = np.mean(val_scores,axis=1)\n",
        "print('average train scores :',avg_train_scores)\n",
        "print('average val scores :',avg_val_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9gwQrJwoMe7",
        "outputId": "cd60a32b-cc48-4a6c-b4de-1bc8d1333c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best alpha from 5 fold CV: 0.01\n"
          ]
        }
      ],
      "source": [
        "#best alpha\n",
        "\n",
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best alpha from 5 fold CV:',best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5SNgEWvZoBXN"
      },
      "outputs": [],
      "source": [
        "linersvc_l2 =LinearSVC(penalty='l2',C = best_alpha)\n",
        "\n",
        "linersvc_l2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train =linersvc_l2.predict(X_train)\n",
        "y_pred_test=linersvc_l2.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(vii))}$"
      ],
      "metadata": {
        "id": "9KMhQTTe6n7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3MOTaLvtzPXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b6317d-7096-4713-a836-323f26e5fc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.85250849 0.84534138 0.85326292 0.84081479 0.83446456]\n",
            " [0.8200679  0.81252358 0.83289325 0.81327801 0.80656109]\n",
            " [0.79818936 0.78800453 0.80045266 0.79102226 0.78431373]\n",
            " [0.77065258 0.76650321 0.78159185 0.77442475 0.76432881]\n",
            " [0.76461713 0.75480951 0.75405507 0.76650321 0.76093514]\n",
            " [0.75141456 0.74311581 0.73255375 0.74726518 0.74208145]\n",
            " [0.75254621 0.74311581 0.72991324 0.74726518 0.73491704]\n",
            " [0.71859676 0.74311581 0.72727273 0.74726518 0.73491704]\n",
            " [0.73330819 0.73745756 0.72727273 0.70992078 0.73491704]\n",
            " [0.73330819 0.73745756 0.72727273 0.72727273 0.73491704]\n",
            " [0.72236892 0.73330819 0.72387778 0.71708789 0.72926094]\n",
            " [0.72274613 0.72274613 0.70614862 0.71633346 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            " [0.71859676 0.7125613  0.70614862 0.71067522 0.70889894]\n",
            " [0.71557903 0.7125613  0.70388533 0.70841192 0.70550528]\n",
            " [0.71369295 0.7125613  0.70049038 0.70313089 0.70324284]\n",
            " [0.71369295 0.7125613  0.70049038 0.70313089 0.70324284]\n",
            " [0.71369295 0.70350811 0.69030555 0.69483214 0.69268477]\n",
            " [0.70275368 0.696341   0.69030555 0.68728782 0.68891403]\n",
            " [0.69558657 0.696341   0.69030555 0.68728782 0.68891403]\n",
            " [0.69558657 0.696341   0.69030555 0.68728782 0.68891403]\n",
            " [0.5397963  0.53941909 0.53941909 0.53941909 0.65045249]]\n",
            "val scores: [[0.82051282 0.79487179 0.83107089 0.79487179 0.85045317]\n",
            " [0.80090498 0.77224736 0.83710407 0.7933635  0.78549849]\n",
            " [0.75414781 0.76923077 0.79638009 0.79638009 0.77794562]\n",
            " [0.7586727  0.74509804 0.78883861 0.78129713 0.74773414]\n",
            " [0.7254902  0.71644042 0.78883861 0.76923077 0.70090634]\n",
            " [0.72096531 0.70588235 0.75565611 0.74660633 0.72507553]\n",
            " [0.72096531 0.70135747 0.73755656 0.72850679 0.69637462]\n",
            " [0.7239819  0.70135747 0.73604827 0.73453997 0.71903323]\n",
            " [0.67873303 0.70135747 0.73755656 0.74811463 0.71903323]\n",
            " [0.7224736  0.70588235 0.73755656 0.71342383 0.71903323]\n",
            " [0.7224736  0.70588235 0.73755656 0.74660633 0.71903323]\n",
            " [0.69381599 0.70437406 0.73604827 0.71342383 0.70694864]\n",
            " [0.70135747 0.70135747 0.7254902  0.71644042 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            " [0.67873303 0.68174962 0.7254902  0.70739065 0.71601208]\n",
            " [0.68778281 0.68174962 0.71945701 0.70286576 0.72356495]\n",
            " [0.67571644 0.68174962 0.73001508 0.71945701 0.71903323]\n",
            " [0.67571644 0.68174962 0.73001508 0.71945701 0.71903323]\n",
            " [0.67571644 0.67269985 0.69532428 0.70739065 0.71601208]\n",
            " [0.67571644 0.66968326 0.69532428 0.70889894 0.70241692]\n",
            " [0.67571644 0.66968326 0.69532428 0.70889894 0.70241692]\n",
            " [0.67571644 0.66968326 0.69532428 0.70889894 0.70241692]\n",
            " [0.53846154 0.53996983 0.53996983 0.53996983 0.67220544]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 0.0\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.82051282 0.79487179 0.83107089 0.79487179 0.85045317]\n",
            "**************************\n",
            "alpha: 0.017241379310344827\n",
            "train scores: [0.85250849 0.84534138 0.85326292 0.84081479 0.83446456]\n",
            "val scores: [0.80090498 0.77224736 0.83710407 0.7933635  0.78549849]\n",
            "**************************\n",
            "alpha: 0.034482758620689655\n",
            "train scores: [0.8200679  0.81252358 0.83289325 0.81327801 0.80656109]\n",
            "val scores: [0.75414781 0.76923077 0.79638009 0.79638009 0.77794562]\n",
            "**************************\n",
            "alpha: 0.05172413793103448\n",
            "train scores: [0.79818936 0.78800453 0.80045266 0.79102226 0.78431373]\n",
            "val scores: [0.7586727  0.74509804 0.78883861 0.78129713 0.74773414]\n",
            "**************************\n",
            "alpha: 0.06896551724137931\n",
            "train scores: [0.77065258 0.76650321 0.78159185 0.77442475 0.76432881]\n",
            "val scores: [0.7254902  0.71644042 0.78883861 0.76923077 0.70090634]\n",
            "**************************\n",
            "alpha: 0.08620689655172414\n",
            "train scores: [0.76461713 0.75480951 0.75405507 0.76650321 0.76093514]\n",
            "val scores: [0.72096531 0.70588235 0.75565611 0.74660633 0.72507553]\n",
            "**************************\n",
            "alpha: 0.10344827586206896\n",
            "train scores: [0.75141456 0.74311581 0.73255375 0.74726518 0.74208145]\n",
            "val scores: [0.72096531 0.70135747 0.73755656 0.72850679 0.69637462]\n",
            "**************************\n",
            "alpha: 0.12068965517241378\n",
            "train scores: [0.75254621 0.74311581 0.72991324 0.74726518 0.73491704]\n",
            "val scores: [0.7239819  0.70135747 0.73604827 0.73453997 0.71903323]\n",
            "**************************\n",
            "alpha: 0.13793103448275862\n",
            "train scores: [0.71859676 0.74311581 0.72727273 0.74726518 0.73491704]\n",
            "val scores: [0.67873303 0.70135747 0.73755656 0.74811463 0.71903323]\n",
            "**************************\n",
            "alpha: 0.15517241379310345\n",
            "train scores: [0.73330819 0.73745756 0.72727273 0.70992078 0.73491704]\n",
            "val scores: [0.7224736  0.70588235 0.73755656 0.71342383 0.71903323]\n",
            "**************************\n",
            "alpha: 0.1724137931034483\n",
            "train scores: [0.73330819 0.73745756 0.72727273 0.72727273 0.73491704]\n",
            "val scores: [0.7224736  0.70588235 0.73755656 0.74660633 0.71903323]\n",
            "**************************\n",
            "alpha: 0.1896551724137931\n",
            "train scores: [0.72236892 0.73330819 0.72387778 0.71708789 0.72926094]\n",
            "val scores: [0.69381599 0.70437406 0.73604827 0.71342383 0.70694864]\n",
            "**************************\n",
            "alpha: 0.20689655172413793\n",
            "train scores: [0.72274613 0.72274613 0.70614862 0.71633346 0.70927602]\n",
            "val scores: [0.70135747 0.70135747 0.7254902  0.71644042 0.71601208]\n",
            "**************************\n",
            "alpha: 0.22413793103448276\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.24137931034482757\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.25862068965517243\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.27586206896551724\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.29310344827586204\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.3103448275862069\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.3275862068965517\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.3448275862068966\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.70992078 0.70927602]\n",
            "val scores: [0.67873303 0.70286576 0.7254902  0.71342383 0.71601208]\n",
            "**************************\n",
            "alpha: 0.3620689655172414\n",
            "train scores: [0.71859676 0.7125613  0.70614862 0.71067522 0.70889894]\n",
            "val scores: [0.67873303 0.68174962 0.7254902  0.70739065 0.71601208]\n",
            "**************************\n",
            "alpha: 0.3793103448275862\n",
            "train scores: [0.71557903 0.7125613  0.70388533 0.70841192 0.70550528]\n",
            "val scores: [0.68778281 0.68174962 0.71945701 0.70286576 0.72356495]\n",
            "**************************\n",
            "alpha: 0.39655172413793105\n",
            "train scores: [0.71369295 0.7125613  0.70049038 0.70313089 0.70324284]\n",
            "val scores: [0.67571644 0.68174962 0.73001508 0.71945701 0.71903323]\n",
            "**************************\n",
            "alpha: 0.41379310344827586\n",
            "train scores: [0.71369295 0.7125613  0.70049038 0.70313089 0.70324284]\n",
            "val scores: [0.67571644 0.68174962 0.73001508 0.71945701 0.71903323]\n",
            "**************************\n",
            "alpha: 0.43103448275862066\n",
            "train scores: [0.71369295 0.70350811 0.69030555 0.69483214 0.69268477]\n",
            "val scores: [0.67571644 0.67269985 0.69532428 0.70739065 0.71601208]\n",
            "**************************\n",
            "alpha: 0.4482758620689655\n",
            "train scores: [0.70275368 0.696341   0.69030555 0.68728782 0.68891403]\n",
            "val scores: [0.67571644 0.66968326 0.69532428 0.70889894 0.70241692]\n",
            "**************************\n",
            "alpha: 0.46551724137931033\n",
            "train scores: [0.69558657 0.696341   0.69030555 0.68728782 0.68891403]\n",
            "val scores: [0.67571644 0.66968326 0.69532428 0.70889894 0.70241692]\n",
            "**************************\n",
            "alpha: 0.48275862068965514\n",
            "train scores: [0.69558657 0.696341   0.69030555 0.68728782 0.68891403]\n",
            "val scores: [0.67571644 0.66968326 0.69532428 0.70889894 0.70241692]\n",
            "**************************\n",
            "alpha: 0.5\n",
            "train scores: [0.5397963  0.53941909 0.53941909 0.53941909 0.65045249]\n",
            "val scores: [0.53846154 0.53996983 0.53996983 0.53996983 0.67220544]\n",
            "**************************\n"
          ]
        }
      ],
      "source": [
        "pipeline = make_pipeline(DecisionTreeClassifier())\n",
        "param_range = np.linspace(0,0.5,30)\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline,\n",
        "                                             X=X_train, y=y_train,\n",
        "                                             cv=5,\n",
        "                                              param_name='decisiontreeclassifier__min_weight_fraction_leaf', param_range=param_range)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param_range)):\n",
        "  print('alpha:', param_range[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "t6J8NqYYzQKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c96f4b-c4b4-42d7-a09c-733f54b99160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_hyper_parameter from 5 fold CV: 0.05172413793103448\n"
          ]
        }
      ],
      "source": [
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best_hyper_parameter from 5 fold CV:',best_alpha)\n",
        "best_hyper_parameter.append(best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SO3DyfP8zTof"
      },
      "outputs": [],
      "source": [
        "decisiontreeclassifier =DecisionTreeClassifier(min_weight_fraction_leaf=best_alpha)\n",
        "\n",
        "decisiontreeclassifier.fit(X_train, y_train)\n",
        "y_pred_train =decisiontreeclassifier.predict(X_train)\n",
        "y_pred_test=decisiontreeclassifier.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model3= SVC(probability=True)\n",
        "\n",
        "# #creating a dictionary of hyper parameters\n",
        "# parameters=[{\"C\":[1,10,100,1000], 'penalty': ['l1']}]\n",
        "\n",
        "# #finding the best score and best hyper parameters using 5 fold cross validation and gridsearchCV for 3 performace measures\n",
        "# for scr in ['accuracy', 'precision_score', 'recall_score']:\n",
        "#   grid_search=GridSearchCV(estimator=model3,param_grid=parameters,scoring=scr,cv=5,n_jobs=-1)\n",
        "#   grid_search=grid_search.fit(X,y)\n",
        "#   print(f\"\\nFor metrics: {scr}\")\n",
        "#   print(f\"The best score is = {grid_search.best_score_} and best parameters are= {grid_search.best_params_}\");"
      ],
      "metadata": {
        "id": "LGDDFJGWY4iw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(vi))}$"
      ],
      "metadata": {
        "id": "5oW9p8Vn6xLL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZIlqgbaVyHtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013d45ca-cd27-4c20-b67f-ab8cace04af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.92342512 0.92078461 0.92417955 0.93059223 0.93514329]\n",
            " [0.93021501 0.92870615 0.93172388 0.93096945 0.9321267 ]\n",
            " [0.91059977 0.90267823 0.90871369 0.90984534 0.90686275]\n",
            " [0.9041871  0.9064504  0.90795926 0.89928329 0.91025641]\n",
            " [0.89287061 0.89437948 0.89890607 0.89437948 0.89253394]\n",
            " [0.8879668  0.88947567 0.89400226 0.89098453 0.89894419]\n",
            " [0.88268578 0.88155413 0.88419464 0.8879668  0.88951735]\n",
            " [0.87740475 0.88193135 0.88909845 0.88381743 0.88159879]\n",
            " [0.87136929 0.87966805 0.87815919 0.88457186 0.87933635]\n",
            " [0.86797435 0.88532629 0.87891362 0.88230856 0.87594268]\n",
            " [0.86948321 0.8785364  0.87250094 0.88570351 0.87518854]\n",
            " [0.8645794  0.87400981 0.87815919 0.87589589 0.86802413]\n",
            " [0.86721992 0.869106   0.87023765 0.87589589 0.8714178 ]\n",
            " [0.86382497 0.86721992 0.87514146 0.87589589 0.8627451 ]\n",
            " [0.86759713 0.86759713 0.87023765 0.87212373 0.86651584]\n",
            " [0.86646548 0.86571105 0.87061486 0.86721992 0.86463047]\n",
            " [0.87250094 0.86344776 0.86156167 0.86533384 0.86500754]\n",
            " [0.87551867 0.85929838 0.86382497 0.86307054 0.86048265]\n",
            " [0.87212373 0.85703508 0.85816673 0.8668427  0.86236802]]\n",
            "val scores: [[0.8612368  0.84012066 0.84162896 0.83861237 0.87009063]\n",
            " [0.8280543  0.82202112 0.79939668 0.81447964 0.8429003 ]\n",
            " [0.84615385 0.85520362 0.84917044 0.8627451  0.86555891]\n",
            " [0.83861237 0.83710407 0.83559578 0.84012066 0.8489426 ]\n",
            " [0.84464555 0.84162896 0.84615385 0.85520362 0.84743202]\n",
            " [0.83107089 0.82051282 0.826546   0.84766214 0.85045317]\n",
            " [0.84464555 0.83710407 0.85218703 0.85369532 0.83232628]\n",
            " [0.84313725 0.83710407 0.826546   0.85369532 0.83836858]\n",
            " [0.84615385 0.83861237 0.82503771 0.84917044 0.83534743]\n",
            " [0.84615385 0.82956259 0.81749623 0.83408748 0.83987915]\n",
            " [0.83257919 0.83408748 0.82202112 0.83710407 0.83685801]\n",
            " [0.83257919 0.82051282 0.81749623 0.82956259 0.83685801]\n",
            " [0.82202112 0.82352941 0.82202112 0.83559578 0.83232628]\n",
            " [0.82202112 0.83408748 0.81749623 0.82202112 0.8429003 ]\n",
            " [0.8280543  0.83861237 0.8280543  0.82956259 0.83987915]\n",
            " [0.82352941 0.83257919 0.82202112 0.83257919 0.84441088]\n",
            " [0.82503771 0.84313725 0.83408748 0.83861237 0.83534743]\n",
            " [0.82352941 0.84162896 0.82352941 0.84162896 0.8429003 ]\n",
            " [0.82202112 0.85067873 0.84162896 0.84012066 0.83232628]\n",
            " [0.82051282 0.84615385 0.83257919 0.82956259 0.83081571]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 1\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.8612368  0.84012066 0.84162896 0.83861237 0.87009063]\n",
            "**************************\n",
            "alpha: 2\n",
            "train scores: [0.92342512 0.92078461 0.92417955 0.93059223 0.93514329]\n",
            "val scores: [0.8280543  0.82202112 0.79939668 0.81447964 0.8429003 ]\n",
            "**************************\n",
            "alpha: 3\n",
            "train scores: [0.93021501 0.92870615 0.93172388 0.93096945 0.9321267 ]\n",
            "val scores: [0.84615385 0.85520362 0.84917044 0.8627451  0.86555891]\n",
            "**************************\n",
            "alpha: 4\n",
            "train scores: [0.91059977 0.90267823 0.90871369 0.90984534 0.90686275]\n",
            "val scores: [0.83861237 0.83710407 0.83559578 0.84012066 0.8489426 ]\n",
            "**************************\n",
            "alpha: 5\n",
            "train scores: [0.9041871  0.9064504  0.90795926 0.89928329 0.91025641]\n",
            "val scores: [0.84464555 0.84162896 0.84615385 0.85520362 0.84743202]\n",
            "**************************\n",
            "alpha: 6\n",
            "train scores: [0.89287061 0.89437948 0.89890607 0.89437948 0.89253394]\n",
            "val scores: [0.83107089 0.82051282 0.826546   0.84766214 0.85045317]\n",
            "**************************\n",
            "alpha: 7\n",
            "train scores: [0.8879668  0.88947567 0.89400226 0.89098453 0.89894419]\n",
            "val scores: [0.84464555 0.83710407 0.85218703 0.85369532 0.83232628]\n",
            "**************************\n",
            "alpha: 8\n",
            "train scores: [0.88268578 0.88155413 0.88419464 0.8879668  0.88951735]\n",
            "val scores: [0.84313725 0.83710407 0.826546   0.85369532 0.83836858]\n",
            "**************************\n",
            "alpha: 9\n",
            "train scores: [0.87740475 0.88193135 0.88909845 0.88381743 0.88159879]\n",
            "val scores: [0.84615385 0.83861237 0.82503771 0.84917044 0.83534743]\n",
            "**************************\n",
            "alpha: 10\n",
            "train scores: [0.87136929 0.87966805 0.87815919 0.88457186 0.87933635]\n",
            "val scores: [0.84615385 0.82956259 0.81749623 0.83408748 0.83987915]\n",
            "**************************\n",
            "alpha: 11\n",
            "train scores: [0.86797435 0.88532629 0.87891362 0.88230856 0.87594268]\n",
            "val scores: [0.83257919 0.83408748 0.82202112 0.83710407 0.83685801]\n",
            "**************************\n",
            "alpha: 12\n",
            "train scores: [0.86948321 0.8785364  0.87250094 0.88570351 0.87518854]\n",
            "val scores: [0.83257919 0.82051282 0.81749623 0.82956259 0.83685801]\n",
            "**************************\n",
            "alpha: 13\n",
            "train scores: [0.8645794  0.87400981 0.87815919 0.87589589 0.86802413]\n",
            "val scores: [0.82202112 0.82352941 0.82202112 0.83559578 0.83232628]\n",
            "**************************\n",
            "alpha: 14\n",
            "train scores: [0.86721992 0.869106   0.87023765 0.87589589 0.8714178 ]\n",
            "val scores: [0.82202112 0.83408748 0.81749623 0.82202112 0.8429003 ]\n",
            "**************************\n",
            "alpha: 15\n",
            "train scores: [0.86382497 0.86721992 0.87514146 0.87589589 0.8627451 ]\n",
            "val scores: [0.8280543  0.83861237 0.8280543  0.82956259 0.83987915]\n",
            "**************************\n",
            "alpha: 16\n",
            "train scores: [0.86759713 0.86759713 0.87023765 0.87212373 0.86651584]\n",
            "val scores: [0.82352941 0.83257919 0.82202112 0.83257919 0.84441088]\n",
            "**************************\n",
            "alpha: 17\n",
            "train scores: [0.86646548 0.86571105 0.87061486 0.86721992 0.86463047]\n",
            "val scores: [0.82503771 0.84313725 0.83408748 0.83861237 0.83534743]\n",
            "**************************\n",
            "alpha: 18\n",
            "train scores: [0.87250094 0.86344776 0.86156167 0.86533384 0.86500754]\n",
            "val scores: [0.82352941 0.84162896 0.82352941 0.84162896 0.8429003 ]\n",
            "**************************\n",
            "alpha: 19\n",
            "train scores: [0.87551867 0.85929838 0.86382497 0.86307054 0.86048265]\n",
            "val scores: [0.82202112 0.85067873 0.84162896 0.84012066 0.83232628]\n",
            "**************************\n",
            "alpha: 20\n",
            "train scores: [0.87212373 0.85703508 0.85816673 0.8668427  0.86236802]\n",
            "val scores: [0.82051282 0.84615385 0.83257919 0.82956259 0.83081571]\n",
            "**************************\n"
          ]
        }
      ],
      "source": [
        "pipeline = make_pipeline(KNeighborsClassifier())\n",
        "param_range = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline,\n",
        "                                             X=X_train, y=y_train,\n",
        "                                             cv=5,\n",
        "                                              param_name='kneighborsclassifier__n_neighbors', param_range=param_range)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param_range)):\n",
        "  print('alpha:', param_range[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "eCnLWzpxy3sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396a0013-36a4-4f91-d571-c209c52ab4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average train scores : [0.99947193 0.92682496 0.93074824 0.90773996 0.90562729 0.89461392\n",
            " 0.89227469 0.88518374 0.88277015 0.87862095 0.8780931  0.87628252\n",
            " 0.87213368 0.87077545 0.86896547 0.8688143  0.86692836 0.86557035\n",
            " 0.86443904 0.86330725]\n",
            "average val scores : [0.85033789 0.82137041 0.85576638 0.8400751  0.8470128  0.83524901\n",
            " 0.84399165 0.83977025 0.83886436 0.83343586 0.83252997 0.82740177\n",
            " 0.82709874 0.82770525 0.83283254 0.83102396 0.83524445 0.83464341\n",
            " 0.83735515 0.83192483]\n"
          ]
        }
      ],
      "source": [
        "#print(train_scores.shape)\n",
        "avg_train_scores = np.mean(train_scores,axis=1)\n",
        "avg_val_scores = np.mean(val_scores,axis=1)\n",
        "print('average train scores :',avg_train_scores)\n",
        "print('average val scores :',avg_val_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ila5gPXZzDTi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84742f59-f6c7-40fd-c712-c68b21499284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best alpha from 10 fold CV: 3\n"
          ]
        }
      ],
      "source": [
        "#best alpha\n",
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best alpha from 10 fold CV:',best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hHTpeTStyc3c"
      },
      "outputs": [],
      "source": [
        "knn =KNeighborsClassifier(n_neighbors=best_alpha)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_train =knn.predict(X_train)\n",
        "y_pred_test=knn.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(ii))}$"
      ],
      "metadata": {
        "id": "vu7vPUck6-pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(LogisticRegression(solver='saga', penalty='l1', random_state=1))\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline, X=X_train, y=y_train, cv=5, param_name='logisticregression__C', param_range=param_range)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param_range)):\n",
        "  print('alpha:', param_range[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zif8Pj3qPX5",
        "outputId": "dd41268c-f578-48e8-cb34-86388ae553ac"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[0.93323274 0.93210109 0.93247831 0.93511882 0.9392911 ]\n",
            " [0.93436439 0.93360996 0.93625047 0.93738212 0.94042232]\n",
            " [0.93587326 0.93436439 0.93813655 0.93775934 0.94193062]\n",
            " [0.93549604 0.93474161 0.9370049  0.93775934 0.94230769]\n",
            " [0.93511882 0.93436439 0.9370049  0.93775934 0.94306184]\n",
            " [0.93511882 0.93511882 0.9370049  0.93775934 0.94306184]\n",
            " [0.93511882 0.93511882 0.9370049  0.93775934 0.94306184]\n",
            " [0.93549604 0.93511882 0.9370049  0.93775934 0.94268477]\n",
            " [0.93511882 0.93511882 0.9370049  0.93775934 0.94193062]\n",
            " [0.93511882 0.93511882 0.9370049  0.93775934 0.94193062]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            " [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]]\n",
            "val scores: [[0.87481146 0.88989442 0.88235294 0.88989442 0.87462236]\n",
            " [0.87481146 0.88989442 0.88536953 0.89441931 0.87613293]\n",
            " [0.87481146 0.88989442 0.88687783 0.89441931 0.88066465]\n",
            " [0.87481146 0.89140271 0.88536953 0.89441931 0.88217523]\n",
            " [0.87481146 0.88989442 0.88536953 0.89441931 0.88217523]\n",
            " [0.87631976 0.88989442 0.88536953 0.89291101 0.88217523]\n",
            " [0.87631976 0.88989442 0.88536953 0.89291101 0.88066465]\n",
            " [0.87631976 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            " [0.87631976 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88838612 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88838612 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88838612 0.88386124 0.89441931 0.88066465]\n",
            " [0.87782805 0.88838612 0.88386124 0.89441931 0.88217523]\n",
            " [0.87782805 0.88838612 0.88386124 0.89441931 0.88217523]\n",
            " [0.87782805 0.88838612 0.88386124 0.89441931 0.88217523]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 1\n",
            "train scores: [0.93323274 0.93210109 0.93247831 0.93511882 0.9392911 ]\n",
            "val scores: [0.87481146 0.88989442 0.88235294 0.88989442 0.87462236]\n",
            "**************************\n",
            "alpha: 2\n",
            "train scores: [0.93436439 0.93360996 0.93625047 0.93738212 0.94042232]\n",
            "val scores: [0.87481146 0.88989442 0.88536953 0.89441931 0.87613293]\n",
            "**************************\n",
            "alpha: 3\n",
            "train scores: [0.93587326 0.93436439 0.93813655 0.93775934 0.94193062]\n",
            "val scores: [0.87481146 0.88989442 0.88687783 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 4\n",
            "train scores: [0.93549604 0.93474161 0.9370049  0.93775934 0.94230769]\n",
            "val scores: [0.87481146 0.89140271 0.88536953 0.89441931 0.88217523]\n",
            "**************************\n",
            "alpha: 5\n",
            "train scores: [0.93511882 0.93436439 0.9370049  0.93775934 0.94306184]\n",
            "val scores: [0.87481146 0.88989442 0.88536953 0.89441931 0.88217523]\n",
            "**************************\n",
            "alpha: 6\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93775934 0.94306184]\n",
            "val scores: [0.87631976 0.88989442 0.88536953 0.89291101 0.88217523]\n",
            "**************************\n",
            "alpha: 7\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93775934 0.94306184]\n",
            "val scores: [0.87631976 0.88989442 0.88536953 0.89291101 0.88066465]\n",
            "**************************\n",
            "alpha: 8\n",
            "train scores: [0.93549604 0.93511882 0.9370049  0.93775934 0.94268477]\n",
            "val scores: [0.87631976 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 9\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93775934 0.94193062]\n",
            "val scores: [0.87631976 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 10\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93775934 0.94193062]\n",
            "val scores: [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 11\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 12\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 13\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 14\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88989442 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 15\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88838612 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 16\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88838612 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 17\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88838612 0.88386124 0.89441931 0.88066465]\n",
            "**************************\n",
            "alpha: 18\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88838612 0.88386124 0.89441931 0.88217523]\n",
            "**************************\n",
            "alpha: 19\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88838612 0.88386124 0.89441931 0.88217523]\n",
            "**************************\n",
            "alpha: 20\n",
            "train scores: [0.93511882 0.93511882 0.9370049  0.93813655 0.94230769]\n",
            "val scores: [0.87782805 0.88838612 0.88386124 0.89441931 0.88217523]\n",
            "**************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_scores.shape)\n",
        "avg_train_scores = np.mean(train_scores,axis=1)\n",
        "avg_val_scores = np.mean(val_scores,axis=1)\n",
        "print('average train scores :',avg_train_scores)\n",
        "print('average val scores :',avg_val_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRHELGdEqRP4",
        "outputId": "c05777ec-b718-46a7-9b67-df3d65d0d3a1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average train scores : [0.93444441 0.93640585 0.93761283 0.93746192 0.93746186 0.93761275\n",
            " 0.93761275 0.93761277 0.9373865  0.9373865  0.93753736 0.93753736\n",
            " 0.93753736 0.93753736 0.93753736 0.93753736 0.93753736 0.93753736\n",
            " 0.93753736 0.93753736]\n",
            "average val scores : [0.88231512 0.88412553 0.88533353 0.88563565 0.88533399 0.88533399\n",
            " 0.88503187 0.88503187 0.88503187 0.88533353 0.88533353 0.88533353\n",
            " 0.88533353 0.88533353 0.88503187 0.88503187 0.88503187 0.88533399\n",
            " 0.88533399 0.88533399]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best alpha\n",
        "\n",
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best alpha from 5 fold CV:',best_alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhxuZEw5qWHg",
        "outputId": "119fa37d-ac34-4b1e-f8b4-fd504e97856a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best alpha from 5 fold CV: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(solver='lbfgs', penalty='l2', C = best_alpha, random_state=1)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_train =clf.predict(X_train)\n",
        "y_pred_test=clf.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfgPIXpTqW3f",
        "outputId": "69a69516-5c0a-4e76-95f5-3813cba1cb2a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(v))}$"
      ],
      "metadata": {
        "id": "bbCbWecP6sUb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "NGEYALXRuArf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0240a1-95ec-4063-df4e-23233131ff70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[0.5397963  0.53941909 0.53941909 0.53941909 0.53959276]\n",
            " [0.82648057 0.81403244 0.80724255 0.81214636 0.81221719]\n",
            " [0.92606564 0.9275745  0.92946058 0.92682007 0.92948718]\n",
            " [0.97019992 0.97095436 0.97170879 0.97057714 0.97473605]\n",
            " [0.98528857 0.98302527 0.98566579 0.98528857 0.98604827]\n",
            " [0.99471897 0.99547341 0.99622784 0.99585062 0.99585219]]\n",
            "val scores: [[0.53846154 0.53996983 0.53996983 0.53996983 0.53927492]\n",
            " [0.76772247 0.79638009 0.82503771 0.81447964 0.79154079]\n",
            " [0.8627451  0.89140271 0.87330317 0.87330317 0.87009063]\n",
            " [0.88989442 0.8974359  0.87631976 0.89140271 0.88217523]\n",
            " [0.87330317 0.87631976 0.87028658 0.88989442 0.86858006]\n",
            " [0.85822021 0.86726998 0.85822021 0.87933635 0.85800604]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 0.01\n",
            "train scores: [0.5397963  0.53941909 0.53941909 0.53941909 0.53959276]\n",
            "val scores: [0.53846154 0.53996983 0.53996983 0.53996983 0.53927492]\n",
            "**************************\n",
            "alpha: 0.1\n",
            "train scores: [0.82648057 0.81403244 0.80724255 0.81214636 0.81221719]\n",
            "val scores: [0.76772247 0.79638009 0.82503771 0.81447964 0.79154079]\n",
            "**************************\n",
            "alpha: 1\n",
            "train scores: [0.92606564 0.9275745  0.92946058 0.92682007 0.92948718]\n",
            "val scores: [0.8627451  0.89140271 0.87330317 0.87330317 0.87009063]\n",
            "**************************\n",
            "alpha: 10\n",
            "train scores: [0.97019992 0.97095436 0.97170879 0.97057714 0.97473605]\n",
            "val scores: [0.88989442 0.8974359  0.87631976 0.89140271 0.88217523]\n",
            "**************************\n",
            "alpha: 100\n",
            "train scores: [0.98528857 0.98302527 0.98566579 0.98528857 0.98604827]\n",
            "val scores: [0.87330317 0.87631976 0.87028658 0.88989442 0.86858006]\n",
            "**************************\n",
            "alpha: 1000\n",
            "train scores: [0.99471897 0.99547341 0.99622784 0.99585062 0.99585219]\n",
            "val scores: [0.85822021 0.86726998 0.85822021 0.87933635 0.85800604]\n",
            "**************************\n"
          ]
        }
      ],
      "source": [
        "pipeline = make_pipeline(SVC(kernel='rbf'))\n",
        "param_range = [1e-2,1e-1,1,10,100,1000]\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline,\n",
        "                                             X=X_train, y=y_train,\n",
        "                                             cv=5,\n",
        "                                              param_name='svc__C', param_range=param_range)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param_range)):\n",
        "  print('alpha:', param_range[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "WHeYy_N5xi7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b9f30a-06e3-446e-d598-fa9002fe8731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average train scores : [0.53952926 0.81442382 0.92788159 0.97163525 0.98506329 0.99562461]\n",
            "average val scores : [0.53952919 0.79903214 0.87416896 0.8874456  0.8756768  0.86421056]\n"
          ]
        }
      ],
      "source": [
        "#print(train_scores.shape)\n",
        "avg_train_scores = np.mean(train_scores,axis=1)\n",
        "avg_val_scores = np.mean(val_scores,axis=1)\n",
        "print('average train scores :',avg_train_scores)\n",
        "print('average val scores :',avg_val_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "GXrXcPGqx3eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28380db8-8808-4456-abbf-d13086dfdf84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best alpha from 10 fold CV: 10\n"
          ]
        }
      ],
      "source": [
        "#best alpha\n",
        "\n",
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best alpha from 10 fold CV:',best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "0IZKSviEx4IC"
      },
      "outputs": [],
      "source": [
        "svc_kernel =SVC(kernel='rbf', C = best_alpha)\n",
        "\n",
        "svc_kernel.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train =svc_kernel.predict(X_train)\n",
        "y_pred_test=svc_kernel.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(viii))}$"
      ],
      "metadata": {
        "id": "ibXy-5xY61EA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "MljQyuyvzZPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6975da40-828c-4525-89f4-0c7e913a0a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[0.99396454 0.99396454 0.99547341 0.99585062 0.99660633]\n",
            " [0.99849114 0.99849114 0.99849114 0.99962278 0.99924585]\n",
            " [0.99886835 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            " [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]]\n",
            "val scores: [[0.81598793 0.82352941 0.82503771 0.80844646 0.83987915]\n",
            " [0.83257919 0.83408748 0.82051282 0.84012066 0.86858006]\n",
            " [0.84313725 0.86425339 0.83559578 0.85671192 0.86404834]\n",
            " [0.84766214 0.8627451  0.85972851 0.85972851 0.87009063]\n",
            " [0.85972851 0.85369532 0.85972851 0.8612368  0.86858006]\n",
            " [0.86576169 0.85822021 0.84162896 0.85972851 0.88519637]\n",
            " [0.84615385 0.84012066 0.85369532 0.85067873 0.8836858 ]\n",
            " [0.87330317 0.85671192 0.85520362 0.86726998 0.87613293]\n",
            " [0.86576169 0.84012066 0.85972851 0.87933635 0.86253776]\n",
            " [0.86576169 0.84917044 0.86877828 0.87028658 0.88670695]\n",
            " [0.87481146 0.85369532 0.86425339 0.87631976 0.87462236]\n",
            " [0.86726998 0.85822021 0.86576169 0.87179487 0.89123867]\n",
            " [0.87330317 0.86726998 0.8612368  0.87631976 0.87613293]\n",
            " [0.87933635 0.87028658 0.87179487 0.86877828 0.88217523]\n",
            " [0.87179487 0.85822021 0.86877828 0.87481146 0.89274924]\n",
            " [0.8627451  0.87330317 0.86425339 0.86576169 0.89123867]\n",
            " [0.86425339 0.86877828 0.88386124 0.86576169 0.88217523]\n",
            " [0.85520362 0.87179487 0.8612368  0.87330317 0.87613293]\n",
            " [0.88386124 0.87179487 0.87631976 0.86726998 0.8776435 ]\n",
            " [0.86726998 0.85671192 0.87933635 0.86877828 0.88217523]\n",
            " [0.86877828 0.87330317 0.87330317 0.86576169 0.88066465]\n",
            " [0.88235294 0.86877828 0.86877828 0.88084465 0.88217523]\n",
            " [0.87179487 0.86425339 0.87782805 0.88084465 0.88670695]\n",
            " [0.87631976 0.86877828 0.87330317 0.87330317 0.88821752]\n",
            " [0.86425339 0.85671192 0.87631976 0.87179487 0.88821752]\n",
            " [0.8627451  0.85972851 0.87631976 0.87631976 0.89274924]\n",
            " [0.87782805 0.86877828 0.86726998 0.87782805 0.8836858 ]\n",
            " [0.86726998 0.85671192 0.86726998 0.87933635 0.88519637]\n",
            " [0.8627451  0.86726998 0.87481146 0.88084465 0.88519637]\n",
            " [0.87028658 0.85822021 0.87330317 0.87330317 0.88670695]\n",
            " [0.87330317 0.87330317 0.87933635 0.87933635 0.87613293]\n",
            " [0.87028658 0.86425339 0.87028658 0.88084465 0.88821752]\n",
            " [0.86576169 0.85520362 0.87179487 0.88235294 0.88670695]\n",
            " [0.85671192 0.8627451  0.86726998 0.87631976 0.8836858 ]\n",
            " [0.87481146 0.86726998 0.86726998 0.87933635 0.89123867]\n",
            " [0.87330317 0.85822021 0.8627451  0.88084465 0.87462236]\n",
            " [0.88235294 0.87028658 0.87933635 0.87782805 0.88670695]\n",
            " [0.86877828 0.86726998 0.87782805 0.87782805 0.89425982]\n",
            " [0.87481146 0.87179487 0.87028658 0.87631976 0.88670695]\n",
            " [0.87028658 0.87179487 0.86726998 0.87933635 0.88519637]\n",
            " [0.87782805 0.86726998 0.87028658 0.87631976 0.8836858 ]\n",
            " [0.87631976 0.86726998 0.87933635 0.88386124 0.88670695]\n",
            " [0.87330317 0.86576169 0.87330317 0.88235294 0.88066465]\n",
            " [0.87631976 0.86425339 0.87782805 0.87782805 0.8897281 ]\n",
            " [0.87330317 0.87028658 0.87782805 0.88235294 0.87613293]\n",
            " [0.87330317 0.87933635 0.87330317 0.87782805 0.88519637]\n",
            " [0.86877828 0.86877828 0.86877828 0.88235294 0.88670695]\n",
            " [0.86726998 0.87028658 0.87330317 0.88536953 0.87915408]\n",
            " [0.87330317 0.87782805 0.88235294 0.87631976 0.8836858 ]\n",
            " [0.87179487 0.86576169 0.88084465 0.88084465 0.88821752]\n",
            " [0.87631976 0.86576169 0.87631976 0.87481146 0.8836858 ]\n",
            " [0.87028658 0.86576169 0.87179487 0.87933635 0.88519637]\n",
            " [0.87631976 0.87179487 0.87933635 0.88235294 0.88217523]\n",
            " [0.87481146 0.86576169 0.88386124 0.88235294 0.88670695]\n",
            " [0.87631976 0.8627451  0.87631976 0.88838612 0.89123867]\n",
            " [0.87179487 0.86726998 0.88084465 0.87782805 0.88519637]\n",
            " [0.87481146 0.86877828 0.87481146 0.87782805 0.88519637]\n",
            " [0.87933635 0.87330317 0.87782805 0.88536953 0.8836858 ]\n",
            " [0.87782805 0.87028658 0.87782805 0.88235294 0.88519637]\n",
            " [0.87631976 0.86576169 0.87933635 0.88386124 0.88519637]\n",
            " [0.87631976 0.87179487 0.87933635 0.87782805 0.88519637]\n",
            " [0.87330317 0.86877828 0.88084465 0.88536953 0.8836858 ]\n",
            " [0.87481146 0.87330317 0.88235294 0.88084465 0.88519637]\n",
            " [0.87481146 0.87028658 0.87631976 0.88536953 0.88821752]\n",
            " [0.87933635 0.87330317 0.87782805 0.88235294 0.8897281 ]\n",
            " [0.87631976 0.87179487 0.87933635 0.88235294 0.8897281 ]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 10\n",
            "train scores: [0.99396454 0.99396454 0.99547341 0.99585062 0.99660633]\n",
            "val scores: [0.81598793 0.82352941 0.82503771 0.80844646 0.83987915]\n",
            "**************************\n",
            "alpha: 20\n",
            "train scores: [0.99849114 0.99849114 0.99849114 0.99962278 0.99924585]\n",
            "val scores: [0.83257919 0.83408748 0.82051282 0.84012066 0.86858006]\n",
            "**************************\n",
            "alpha: 30\n",
            "train scores: [0.99886835 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.84313725 0.86425339 0.83559578 0.85671192 0.86404834]\n",
            "**************************\n",
            "alpha: 40\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.84766214 0.8627451  0.85972851 0.85972851 0.87009063]\n",
            "**************************\n",
            "alpha: 50\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.85972851 0.85369532 0.85972851 0.8612368  0.86858006]\n",
            "**************************\n",
            "alpha: 60\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86576169 0.85822021 0.84162896 0.85972851 0.88519637]\n",
            "**************************\n",
            "alpha: 70\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.84615385 0.84012066 0.85369532 0.85067873 0.8836858 ]\n",
            "**************************\n",
            "alpha: 80\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.85671192 0.85520362 0.86726998 0.87613293]\n",
            "**************************\n",
            "alpha: 90\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86576169 0.84012066 0.85972851 0.87933635 0.86253776]\n",
            "**************************\n",
            "alpha: 100\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86576169 0.84917044 0.86877828 0.87028658 0.88670695]\n",
            "**************************\n",
            "alpha: 110\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87481146 0.85369532 0.86425339 0.87631976 0.87462236]\n",
            "**************************\n",
            "alpha: 120\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86726998 0.85822021 0.86576169 0.87179487 0.89123867]\n",
            "**************************\n",
            "alpha: 130\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.86726998 0.8612368  0.87631976 0.87613293]\n",
            "**************************\n",
            "alpha: 140\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87933635 0.87028658 0.87179487 0.86877828 0.88217523]\n",
            "**************************\n",
            "alpha: 150\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87179487 0.85822021 0.86877828 0.87481146 0.89274924]\n",
            "**************************\n",
            "alpha: 160\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.8627451  0.87330317 0.86425339 0.86576169 0.89123867]\n",
            "**************************\n",
            "alpha: 170\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86425339 0.86877828 0.88386124 0.86576169 0.88217523]\n",
            "**************************\n",
            "alpha: 180\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.85520362 0.87179487 0.8612368  0.87330317 0.87613293]\n",
            "**************************\n",
            "alpha: 190\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.88386124 0.87179487 0.87631976 0.86726998 0.8776435 ]\n",
            "**************************\n",
            "alpha: 200\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86726998 0.85671192 0.87933635 0.86877828 0.88217523]\n",
            "**************************\n",
            "alpha: 210\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86877828 0.87330317 0.87330317 0.86576169 0.88066465]\n",
            "**************************\n",
            "alpha: 220\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.88235294 0.86877828 0.86877828 0.88084465 0.88217523]\n",
            "**************************\n",
            "alpha: 230\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87179487 0.86425339 0.87782805 0.88084465 0.88670695]\n",
            "**************************\n",
            "alpha: 240\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.86877828 0.87330317 0.87330317 0.88821752]\n",
            "**************************\n",
            "alpha: 250\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86425339 0.85671192 0.87631976 0.87179487 0.88821752]\n",
            "**************************\n",
            "alpha: 260\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.8627451  0.85972851 0.87631976 0.87631976 0.89274924]\n",
            "**************************\n",
            "alpha: 270\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87782805 0.86877828 0.86726998 0.87782805 0.8836858 ]\n",
            "**************************\n",
            "alpha: 280\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86726998 0.85671192 0.86726998 0.87933635 0.88519637]\n",
            "**************************\n",
            "alpha: 290\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.8627451  0.86726998 0.87481146 0.88084465 0.88519637]\n",
            "**************************\n",
            "alpha: 300\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87028658 0.85822021 0.87330317 0.87330317 0.88670695]\n",
            "**************************\n",
            "alpha: 310\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.87330317 0.87933635 0.87933635 0.87613293]\n",
            "**************************\n",
            "alpha: 320\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87028658 0.86425339 0.87028658 0.88084465 0.88821752]\n",
            "**************************\n",
            "alpha: 330\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86576169 0.85520362 0.87179487 0.88235294 0.88670695]\n",
            "**************************\n",
            "alpha: 340\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.85671192 0.8627451  0.86726998 0.87631976 0.8836858 ]\n",
            "**************************\n",
            "alpha: 350\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87481146 0.86726998 0.86726998 0.87933635 0.89123867]\n",
            "**************************\n",
            "alpha: 400\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.85822021 0.8627451  0.88084465 0.87462236]\n",
            "**************************\n",
            "alpha: 450\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.88235294 0.87028658 0.87933635 0.87782805 0.88670695]\n",
            "**************************\n",
            "alpha: 500\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86877828 0.86726998 0.87782805 0.87782805 0.89425982]\n",
            "**************************\n",
            "alpha: 550\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87481146 0.87179487 0.87028658 0.87631976 0.88670695]\n",
            "**************************\n",
            "alpha: 600\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87028658 0.87179487 0.86726998 0.87933635 0.88519637]\n",
            "**************************\n",
            "alpha: 650\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87782805 0.86726998 0.87028658 0.87631976 0.8836858 ]\n",
            "**************************\n",
            "alpha: 700\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.86726998 0.87933635 0.88386124 0.88670695]\n",
            "**************************\n",
            "alpha: 750\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.86576169 0.87330317 0.88235294 0.88066465]\n",
            "**************************\n",
            "alpha: 800\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.86425339 0.87782805 0.87782805 0.8897281 ]\n",
            "**************************\n",
            "alpha: 850\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.87028658 0.87782805 0.88235294 0.87613293]\n",
            "**************************\n",
            "alpha: 900\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.87933635 0.87330317 0.87782805 0.88519637]\n",
            "**************************\n",
            "alpha: 950\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86877828 0.86877828 0.86877828 0.88235294 0.88670695]\n",
            "**************************\n",
            "alpha: 1000\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.86726998 0.87028658 0.87330317 0.88536953 0.87915408]\n",
            "**************************\n",
            "alpha: 1050\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.87782805 0.88235294 0.87631976 0.8836858 ]\n",
            "**************************\n",
            "alpha: 1100\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87179487 0.86576169 0.88084465 0.88084465 0.88821752]\n",
            "**************************\n",
            "alpha: 1150\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.86576169 0.87631976 0.87481146 0.8836858 ]\n",
            "**************************\n",
            "alpha: 1200\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87028658 0.86576169 0.87179487 0.87933635 0.88519637]\n",
            "**************************\n",
            "alpha: 1300\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.87179487 0.87933635 0.88235294 0.88217523]\n",
            "**************************\n",
            "alpha: 1400\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87481146 0.86576169 0.88386124 0.88235294 0.88670695]\n",
            "**************************\n",
            "alpha: 1500\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.8627451  0.87631976 0.88838612 0.89123867]\n",
            "**************************\n",
            "alpha: 1600\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87179487 0.86726998 0.88084465 0.87782805 0.88519637]\n",
            "**************************\n",
            "alpha: 1700\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87481146 0.86877828 0.87481146 0.87782805 0.88519637]\n",
            "**************************\n",
            "alpha: 1800\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87933635 0.87330317 0.87782805 0.88536953 0.8836858 ]\n",
            "**************************\n",
            "alpha: 1900\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87782805 0.87028658 0.87782805 0.88235294 0.88519637]\n",
            "**************************\n",
            "alpha: 2000\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.86576169 0.87933635 0.88386124 0.88519637]\n",
            "**************************\n",
            "alpha: 2500\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.87179487 0.87933635 0.87782805 0.88519637]\n",
            "**************************\n",
            "alpha: 3000\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87330317 0.86877828 0.88084465 0.88536953 0.8836858 ]\n",
            "**************************\n",
            "alpha: 3500\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87481146 0.87330317 0.88235294 0.88084465 0.88519637]\n",
            "**************************\n",
            "alpha: 4000\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87481146 0.87028658 0.87631976 0.88536953 0.88821752]\n",
            "**************************\n",
            "alpha: 4500\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87933635 0.87330317 0.87782805 0.88235294 0.8897281 ]\n",
            "**************************\n",
            "alpha: 5000\n",
            "train scores: [0.99924557 0.99924557 0.99962278 0.99962278 0.99962293]\n",
            "val scores: [0.87631976 0.87179487 0.87933635 0.88235294 0.8897281 ]\n",
            "**************************\n"
          ]
        }
      ],
      "source": [
        "pipeline = make_pipeline(RandomForestClassifier())\n",
        "param_range = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1300,1400,1500,1600,1700,1800,1900,2000,2500,3000,3500,4000,4500,5000]\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline,\n",
        "                                             X=X_train, y=y_train,\n",
        "                                             cv=5,\n",
        "                                              param_name='randomforestclassifier__n_estimators', param_range=param_range)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param_range)):\n",
        "  print('alpha:', param_range[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "YxnwsidIzbwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ebf4cd-7e39-4285-9f83-eb89787803a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average train scores : [0.99517189 0.99886841 0.99939648 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193\n",
            " 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193 0.99947193]\n",
            "average val scores : [0.82257613 0.83917604 0.85274934 0.85999098 0.86059384 0.86210715\n",
            " 0.85486687 0.86572432 0.86149699 0.86814079 0.86874046 0.87085709\n",
            " 0.87085253 0.87447426 0.87327081 0.8714604  0.87296597 0.86753428\n",
            " 0.87537787 0.87085435 0.87236219 0.87658587 0.87628558 0.87598438\n",
            " 0.87145949 0.87357247 0.87507803 0.87115692 0.87417351 0.87236401\n",
            " 0.87628239 0.87477774 0.87236401 0.86934651 0.87598529 0.8699471\n",
            " 0.87930217 0.87719284 0.87598392 0.87477683 0.87507803 0.87869886\n",
            " 0.87507712 0.87719147 0.87598073 0.87779342 0.87507895 0.87507667\n",
            " 0.87869794 0.87749267 0.87537969 0.87447517 0.87839583 0.87869886\n",
            " 0.87900188 0.87658679 0.87628513 0.87990458 0.8786984  0.87809508\n",
            " 0.87809508 0.87839629 0.87930172 0.87900097 0.88050972 0.8799064 ]\n"
          ]
        }
      ],
      "source": [
        "avg_train_scores = np.mean(train_scores,axis=1)\n",
        "avg_val_scores = np.mean(val_scores,axis=1)\n",
        "print('average train scores :',avg_train_scores)\n",
        "print('average val scores :',avg_val_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "CciFoGFczeV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03c9284-bbf3-4336-d459-84b07cf3b750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_hyper_parameter from 5 fold CV: 4500\n"
          ]
        }
      ],
      "source": [
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best_hyper_parameter from 5 fold CV:',best_alpha)\n",
        "best_hyper_parameter.append(best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "CB80_ps1zgmt"
      },
      "outputs": [],
      "source": [
        "randomforest =RandomForestClassifier(n_estimators=best_alpha)\n",
        "\n",
        "randomforest.fit(X_train, y_train)\n",
        "y_pred_train =randomforest.predict(X_train)\n",
        "y_pred_test=randomforest.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Sequence in which I ran the parts of (d): (i), (iii), (vii), (vi), (ii), (v), (viii), (iv) and the metrics are stored in the list in this sequence only.*"
      ],
      "metadata": {
        "id": "eo5wj6eVF1yW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(e)}$"
      ],
      "metadata": {
        "id": "xGXkPj3P8UMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "aTwW5mr4_nob"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating table for the chi-square test data\n",
        "all_data = [['Accuracy_train', 'Precision_train', 'Recall_train', 'Specificity_train', 'Sensitivity_train', 'Accuracy_test', 'Precision_test', 'Recall_test', 'Specificity_test', 'Sensitivity_test']]\n",
        "for i in  range(len(accuracy_train)):\n",
        "    all_data.append([accuracy_train[i] , precision_train[i] , recall_train[i] , specifity_train[i], senstivity_train[i],  accuracy_test[i] , precision_test[i] ,  recall_test[i] , specifity_test[i], senstivity_test[i]])\n",
        "#all_data.append(['Total', '100', '100', '13.2'])    \n",
        "print(tabulate(all_data, headers = \"firstrow\",tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6h1gNQ97oHN",
        "outputId": "2d74f295-1598-4b9b-f49c-5872389d8f00"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n",
            "|   Accuracy_train |   Precision_train |   Recall_train |   Specificity_train |   Sensitivity_train |   Accuracy_test |   Precision_test |   Recall_test |   Specificity_test |   Sensitivity_test |\n",
            "+==================+===================+================+=====================+=====================+=================+==================+===============+====================+====================+\n",
            "|         0.933011 |          0.920516 |       0.958613 |            0.903014 |            0.958613 |        0.887817 |         0.869663 |      0.917062 |           0.857494 |           0.917062 |\n",
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n",
            "|         0.931201 |          0.917559 |       0.958613 |            0.899083 |            0.958613 |        0.881785 |         0.864865 |      0.909953 |           0.85258  |           0.909953 |\n",
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n",
            "|         0.779421 |          0.759961 |       0.864094 |            0.68021  |            0.864094 |        0.797346 |         0.766807 |      0.864929 |           0.727273 |           0.864929 |\n",
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n",
            "|         0.93452  |          0.928065 |       0.952461 |            0.913499 |            0.952461 |        0.878166 |         0.868966 |      0.895735 |           0.859951 |           0.895735 |\n",
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n",
            "|         0.960169 |          0.956954 |       0.969799 |            0.948886 |            0.969799 |        0.87696  |         0.872093 |      0.888626 |           0.864865 |           0.888626 |\n",
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n",
            "|         0.970127 |          0.961223 |       0.98434  |            0.953473 |            0.98434  |        0.884198 |         0.857456 |      0.92654  |           0.840295 |           0.92654  |\n",
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n",
            "|         0.999396 |          0.999441 |       0.999441 |            0.999345 |            0.999441 |        0.881785 |         0.8375   |      0.952607 |           0.808354 |           0.952607 |\n",
            "+------------------+-------------------+----------------+---------------------+---------------------+-----------------+------------------+---------------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "For training data:\n",
        "\n",
        "* Every metric is highest for random forest model.\n",
        "* All the metrics for decision tree coming out to be the lowest\n",
        "* Recall is same for Logistic with L2 regularizer and SVM with L2 regularizer.\n",
        "\n",
        "For testing data:\n",
        "\n",
        "* Accuracy is almost same for Logistic and SVM with L2 regularizers, random forest and kernel SVM with rbf kernel.\n",
        "* Precision is highest for Logistic with L1 regularizer and lowest for decision tree.\n",
        "* Recall is highest for random forest and almost same for Logistic with L2 regularizer and SVM with L2 regularizer.\n",
        "* Sensitivity is highest for random forest.\n",
        "* Specificity is almost same for Logistic with L2 regularizer and SVM with L2 and kernel SVM with rbf kernel and highest for Logistic with L1 regularizer."
      ],
      "metadata": {
        "id": "Ps9mawzuC7Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(f)}$"
      ],
      "metadata": {
        "id": "dagyrrq4K0-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression and SVM, L1 regularization can lead to sparse models, where many of the features are assigned zero weights, effectively reducing the number of features used in the model. This sparsity can be desirable in some cases, such as when dealing with high-dimensional data with many irrelevant features. L2 regularization, on the other hand, tends to distribute the weight values more evenly across all the features, resulting in dense models that use all the available features."
      ],
      "metadata": {
        "id": "3V8DL-PcqpZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\Large\\textbf{Part(d(iv))}$"
      ],
      "metadata": {
        "id": "_NGqxBAr65R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param = [1,10,100,1000]"
      ],
      "metadata": {
        "id": "Bh3eRbUTYfiq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Bn3VTa76tGVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ccb321-4fc8-4193-84dd-e34238d876b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train scores: [[nan nan nan nan nan]\n",
            " [nan nan nan nan nan]\n",
            " [nan nan nan nan nan]\n",
            " [nan nan nan nan nan]]\n",
            "val scores: [[nan nan nan nan nan]\n",
            " [nan nan nan nan nan]\n",
            " [nan nan nan nan nan]\n",
            " [nan nan nan nan nan]]\n",
            "Printing more details of scores for each alpha:\n",
            "alpha: 1\n",
            "train scores: [nan nan nan nan nan]\n",
            "val scores: [nan nan nan nan nan]\n",
            "**************************\n",
            "alpha: 10\n",
            "train scores: [nan nan nan nan nan]\n",
            "val scores: [nan nan nan nan nan]\n",
            "**************************\n",
            "alpha: 100\n",
            "train scores: [nan nan nan nan nan]\n",
            "val scores: [nan nan nan nan nan]\n",
            "**************************\n",
            "alpha: 1000\n",
            "train scores: [nan nan nan nan nan]\n",
            "val scores: [nan nan nan nan nan]\n",
            "**************************\n"
          ]
        }
      ],
      "source": [
        "pipeline = make_pipeline(LinearSVC(penalty='l1'))\n",
        "train_scores, val_scores = validation_curve(estimator=pipeline, X=X_train, y=y_train, cv=5, param_name='linearsvc__C', param_range=param)\n",
        "print('train scores:',train_scores)\n",
        "print('val scores:',val_scores)\n",
        "\n",
        "print('Printing more details of scores for each alpha:')\n",
        "for i in range(len(param)):\n",
        "  print('alpha:', param[i])\n",
        "  print('train scores:', train_scores[i])\n",
        "  print('val scores:', val_scores[i])\n",
        "  print('**************************')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "O6eAo7lvtLEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c933e9d4-27f8-4c16-8b1d-1437d25137f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average train scores : [nan nan nan nan]\n",
            "average val scores : [nan nan nan nan]\n"
          ]
        }
      ],
      "source": [
        "#print(train_scores.shape)\n",
        "avg_train_scores = np.mean(train_scores,axis=1)\n",
        "avg_val_scores = np.mean(val_scores,axis=1)\n",
        "print('average train scores :',avg_train_scores)\n",
        "print('average val scores :',avg_val_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "4pjfw7UltSia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db34659-2fa5-4c08-a1ab-e073a1a28986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best alpha from 5 fold CV: 10\n"
          ]
        }
      ],
      "source": [
        "#best alpha\n",
        "\n",
        "best_alpha = param_range[np.argmax(avg_val_scores)]\n",
        "print('best alpha from 5 fold CV:',best_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "0q6QBGB8tnq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "852936cb-774b-41ea-cb90-9e44b5057a75"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-01fa154d2e83>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlinersvc_l1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlinersvc_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlinersvc_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[1;32m   1225\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[0;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msolver_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1063\u001b[0m         \u001b[0;34m\"Unsupported set of arguments: %s, Parameters: penalty=%r, loss=%r, dual=%r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True"
          ]
        }
      ],
      "source": [
        "linersvc_l1 =LinearSVC(penalty='l1',C = best_alpha)\n",
        "\n",
        "linersvc_l1.fit(X_train, y_train)\n",
        "\n",
        "y_pred_train =linersvc_l1.predict(X_train)\n",
        "y_pred_test=linersvc_l1.predict(X_test)\n",
        "metrics(X_train,y_train,X_test,y_test,y_pred_train,y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft margin SVM wiht L1 regularizers ended up with nan values."
      ],
      "metadata": {
        "id": "6grs9Q44FtqN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}